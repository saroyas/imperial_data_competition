{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "np.set_printoptions(suppress=True)\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "from tensorflow.keras import regularizers, layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "data_un = pd.read_csv('train_data.csv', sep = ',')\n",
    "\n",
    "#need to check exactly how these are loaded.\n",
    "x_data_uf = data_un.iloc[:,2:36]\n",
    "y_data = data_un.iloc[:, 36:38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning/ normlisation\n",
    "\n",
    "cat_to_int_dict = {'pol_coverage': {'Mini':0, 'Median1':1, 'Median2':2, 'Maxi':3},\n",
    "                           'pol_pay_freq': {'Monthly':0, 'Quarterly':1, 'Biannual':2, 'Yearly':3},\n",
    "                           'pol_payd': {'No':0, 'Yes':1},\n",
    "                           'pol_usage': {'Retired':0, 'WorkPrivate':1, 'Professional':2, 'AllTrips':3},\n",
    "                           'drv_drv2': {'No':0, 'Yes':1},\n",
    "                           'drv_sex1': {'F':0, 'M':1},\n",
    "                           'drv_sex2': {'F':-1, 'M':1, None:0},\n",
    "                           'vh_type': {'Tourism':0, 'Commercial':1,}\n",
    "                           }\n",
    "\n",
    "def car_make_categories(car_make):\n",
    "    if car_make in ['RENAULT','RENAULT','PEUGEOT','CITROEN','VOLKSWAGEN','FORD']:\n",
    "        return car_make\n",
    "    else:\n",
    "        return 'OTHER'\n",
    "    \n",
    "def missing_geo_data(x):\n",
    "    if x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def zero_vh_weight(weight, avg_weight):\n",
    "    if weight<100:\n",
    "        return avg_weight\n",
    "    else:\n",
    "        return weight\n",
    "\n",
    "x_data_f = x_data_uf.replace(cat_to_int_dict, inplace=False)\n",
    "\n",
    "x_data_f.vh_make = x_data_uf['vh_make'].apply(lambda x: car_make_categories(x))\n",
    "\n",
    "avg_vh_weight = x_data_uf['vh_weight'].mean()\n",
    "\n",
    "x_data_f.vh_weight = x_data_uf['vh_weight'].apply(lambda x: zero_vh_weight(x, avg_vh_weight))\n",
    "\n",
    "vh_make_cols = pd.get_dummies(x_data_f.vh_make)\n",
    "\n",
    "vh_fuel_cols= pd.get_dummies(x_data_f.vh_fuel)\n",
    "\n",
    "city_dist_cols= pd.get_dummies(x_data_f.city_district_code)\n",
    "\n",
    "geo_na_col = (x_data_f['population'].isnull()).apply(lambda x: missing_geo_data(x))\n",
    "\n",
    "#need to change means so not dependent on training data\n",
    "MEANS = x_data_uf.mean()\n",
    "x_data_f = x_data_f.fillna(MEANS)\n",
    "\n",
    "\n",
    "cols_to_drop = ['pol_insee_code', 'vh_model', 'regional_department_code', 'commune_code', \n",
    "                'canton_code', 'vh_make', 'city_district_code', 'vh_fuel']\n",
    "x_data_f = x_data_f.drop(cols_to_drop, axis = 1)\n",
    "\n",
    "\n",
    "#replace vh_make with indicators\n",
    "x_data_f = pd.concat([x_data_f, vh_make_cols, vh_fuel_cols], axis=1, sort=False)\n",
    "\n",
    "\n",
    "x_norm_temp = x_data_f.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x_norm_temp)\n",
    "x_norm_temp = pd.DataFrame(x_scaled)\n",
    "\n",
    "x_data = x_norm_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X_data, y_data):\n",
    "    predictions = model.predict(X_data).reshape(X_data.shape[0])\n",
    "    return (predictions - y_data).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The different models I want to test\n",
    "adam_opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "def build_simple_linear_mae():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(1, input_shape=[X_train.shape[1]])\n",
    "  ])\n",
    "\n",
    "  optimizer = adam_opt\n",
    "\n",
    "  model.compile(loss='mae',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "  return model\n",
    "\n",
    "#===\n",
    "def build_simple_linear_mse():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(1, input_shape=[X_train.shape[1]])\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "  return model\n",
    "\n",
    "#===\n",
    "def build_explicit_linear_mse():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(1, activation = 'linear', input_shape=[X_train.shape[1]])\n",
    "  ])\n",
    "\n",
    "  optimizer = adam_opt\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "  return model\n",
    "\n",
    "#====\n",
    "def build_tanh6_relu5_lin():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(6, activation='tanh', input_shape=[X_train.shape[1]]),\n",
    "    layers.Dense(5, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = adam_opt\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "  return model\n",
    "\n",
    "\n",
    "#====\n",
    "def build_tanh15_relu5_relu5_lin():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(15, activation='tanh', input_shape=[X_train.shape[1]]),\n",
    "    layers.Dense(10, activation='relu'),\n",
    "    layers.Dense(5, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "  return model\n",
    "\n",
    "\n",
    "#====\n",
    "def build_tanh15_relu10_lin5_lin():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(15, activation='tanh', input_shape=[X_train.shape[1]]),\n",
    "    layers.Dense(10, activation='relu'),\n",
    "    layers.Dense(5, activation = 'linear'),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = 'sgd'\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, epochs = 1000):\n",
    "\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs=EPOCHS, validation_split = 0.2, verbose=1, \n",
    "                        callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_whole_model_assesment(model, X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test):\n",
    "    try:\n",
    "        model, history = train_model(model, X_train, y_train.claim_amount)\n",
    "        print('TRAINING ENDED')\n",
    "    except:\n",
    "        print('failed training')\n",
    "    try:\n",
    "        plotter.plot({'Basic': history}, metric = \"mse\")\n",
    "    except:\n",
    "        print('failed to print graph - not that its that good anyway')\n",
    "    print('-------------------------------------------------------------------------------')\n",
    "    print('/n')\n",
    "    print('Strategy net profitability')\n",
    "    try:\n",
    "        print(test_model(model, X_test, y_test.claim_amount))\n",
    "    except:\n",
    "        print('failed to test the model')\n",
    "    print('-------------------------------------------------------------------------------')\n",
    "    print('/n')\n",
    "    print('the first few reults of our predictions - to check they arent identical')\n",
    "    try:\n",
    "        print(model.predict(X_test)[:10])\n",
    "    except:\n",
    "        print('failed to make a few quick predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 46800 samples, validate on 11700 samples\n",
      "Epoch 1/1000\n",
      "46496/46800 [============================>.] - ETA: 0s - loss: nan - mae: nan - mse: nan\n",
      "Epoch: 0, loss:nan,  mae:nan,  mse:nan,  val_loss:nan,  val_mae:nan,  val_mse:nan,  \n",
      "46800/46800 [==============================] - 11s 227us/sample - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 2/1000\n",
      "46800/46800 [==============================] - 10s 223us/sample - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 3/1000\n",
      "46800/46800 [==============================] - 11s 226us/sample - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 4/1000\n",
      "46800/46800 [==============================] - 11s 234us/sample - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 5/1000\n",
      "46800/46800 [==============================] - 11s 245us/sample - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 6/1000\n",
      "46800/46800 [==============================] - 13s 281us/sample - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 7/1000\n",
      "46800/46800 [==============================] - 12s 258us/sample - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 8/1000\n",
      "46800/46800 [==============================] - 13s 272us/sample - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 9/1000\n",
      "46800/46800 [==============================] - 12s 263us/sample - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "Epoch 10/1000\n",
      "46800/46800 [==============================] - 14s 299us/sample - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
      "TRAINING ENDED\n",
      "failed to print graph - not that its that good anyway\n",
      "-------------------------------------------------------------------------------\n",
      "/n\n",
      "Strategy net profitability\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "0.0\n",
      "-------------------------------------------------------------------------------\n",
      "/n\n",
      "the first few reults of our predictions - to check they arent identical\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_whole_model_assesment(build_tanh15_relu10_lin5_lin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 46800 samples, validate on 11700 samples\n",
      "Epoch 1/1000\n",
      "44832/46800 [===========================>..] - ETA: 0s - loss: 116.0136 - mae: 116.0137 - mse: 1701655.7500\n",
      "Epoch: 0, loss:115.8704,  mae:115.8704,  mse:1678143.5000,  val_loss:100.5684,  val_mae:100.5684,  val_mse:240338.3281,  \n",
      "46800/46800 [==============================] - 2s 35us/sample - loss: 115.8704 - mae: 115.8704 - mse: 1678143.5000 - val_loss: 100.5684 - val_mae: 100.5684 - val_mse: 240338.3281\n",
      "Epoch 2/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 115.8491 - mae: 115.8490 - mse: 1678144.5000 - val_loss: 100.5636 - val_mae: 100.5637 - val_mse: 240339.2812\n",
      "Epoch 3/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 115.8467 - mae: 115.8465 - mse: 1678144.8750 - val_loss: 100.5634 - val_mae: 100.5634 - val_mse: 240338.5781\n",
      "Epoch 4/1000\n",
      "46800/46800 [==============================] - 1s 29us/sample - loss: 115.8464 - mae: 115.8464 - mse: 1678145.3750 - val_loss: 100.5628 - val_mae: 100.5627 - val_mse: 240339.0312\n",
      "Epoch 5/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 115.8464 - mae: 115.8463 - mse: 1678142.8750 - val_loss: 100.5625 - val_mae: 100.5625 - val_mse: 240338.9688\n",
      "Epoch 6/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 115.8464 - mae: 115.8464 - mse: 1678145.2500 - val_loss: 100.5631 - val_mae: 100.5631 - val_mse: 240339.2344\n",
      "Epoch 7/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 115.8465 - mae: 115.8464 - mse: 1678144.3750 - val_loss: 100.5638 - val_mae: 100.5638 - val_mse: 240339.4531\n",
      "Epoch 8/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 115.8464 - mae: 115.8465 - mse: 1678144.6250 - val_loss: 100.5624 - val_mae: 100.5624 - val_mse: 240338.9219\n",
      "Epoch 9/1000\n",
      "46800/46800 [==============================] - 2s 35us/sample - loss: 115.8464 - mae: 115.8464 - mse: 1678144.5000 - val_loss: 100.5623 - val_mae: 100.5623 - val_mse: 240338.9531\n",
      "Epoch 10/1000\n",
      "46800/46800 [==============================] - 2s 39us/sample - loss: 115.8464 - mae: 115.8464 - mse: 1678144.3750 - val_loss: 100.5624 - val_mae: 100.5624 - val_mse: 240339.0781\n",
      "Epoch 11/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 115.8464 - mae: 115.8464 - mse: 1678144.5000 - val_loss: 100.5626 - val_mae: 100.5626 - val_mse: 240338.8594\n",
      "Epoch 12/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 115.8464 - mae: 115.8463 - mse: 1678145.2500 - val_loss: 100.5624 - val_mae: 100.5623 - val_mse: 240338.9219\n",
      "Epoch 13/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 115.8465 - mae: 115.8465 - mse: 1678141.1250 - val_loss: 100.5627 - val_mae: 100.5627 - val_mse: 240338.9062\n",
      "Epoch 14/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 115.8464 - mae: 115.8464 - mse: 1678143.7500 - val_loss: 100.5623 - val_mae: 100.5623 - val_mse: 240339.0156\n",
      "Epoch 15/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 115.8464 - mae: 115.8463 - mse: 1678144.1250 - val_loss: 100.5624 - val_mae: 100.5625 - val_mse: 240338.9844\n",
      "Epoch 16/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 115.8464 - mae: 115.8462 - mse: 1678142.7500 - val_loss: 100.5629 - val_mae: 100.5629 - val_mse: 240338.6875\n",
      "Epoch 17/1000\n",
      "46800/46800 [==============================] - 2s 35us/sample - loss: 115.8464 - mae: 115.8464 - mse: 1678144.8750 - val_loss: 100.5624 - val_mae: 100.5625 - val_mse: 240338.9219\n",
      "Epoch 18/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 115.8464 - mae: 115.8462 - mse: 1678142.2500 - val_loss: 100.5624 - val_mae: 100.5624 - val_mse: 240338.9531\n",
      "Epoch 19/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 115.8464 - mae: 115.8463 - mse: 1678143.6250 - val_loss: 100.5622 - val_mae: 100.5621 - val_mse: 240338.8750\n",
      "Epoch 20/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 115.8464 - mae: 115.8464 - mse: 1678144.8750 - val_loss: 100.5625 - val_mae: 100.5625 - val_mse: 240338.8594\n",
      "Epoch 21/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 115.8464 - mae: 115.8464 - mse: 1678144.5000 - val_loss: 100.5622 - val_mae: 100.5622 - val_mse: 240338.9219\n",
      "Epoch 22/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 115.8464 - mae: 115.8464 - mse: 1678143.6250 - val_loss: 100.5624 - val_mae: 100.5624 - val_mse: 240338.8125\n",
      "Epoch 23/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 115.8465 - mae: 115.8463 - mse: 1678145.2500 - val_loss: 100.5625 - val_mae: 100.5625 - val_mse: 240339.0781\n",
      "Epoch 24/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 115.8464 - mae: 115.8465 - mse: 1678143.5000 - val_loss: 100.5623 - val_mae: 100.5623 - val_mse: 240338.8750\n",
      "Epoch 25/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 115.8464 - mae: 115.8464 - mse: 1678143.7500 - val_loss: 100.5623 - val_mae: 100.5624 - val_mse: 240338.8750\n",
      "Epoch 26/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 115.8464 - mae: 115.8465 - mse: 1678143.7500 - val_loss: 100.5631 - val_mae: 100.5632 - val_mse: 240338.6875\n",
      "Epoch 27/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 115.8464 - mae: 115.8464 - mse: 1678146.2500 - val_loss: 100.5627 - val_mae: 100.5627 - val_mse: 240338.8594\n",
      "Epoch 28/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 115.8465 - mae: 115.8465 - mse: 1678142.3750 - val_loss: 100.5624 - val_mae: 100.5624 - val_mse: 240339.0781\n",
      "Epoch 29/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 115.8464 - mae: 115.8463 - mse: 1678144.3750 - val_loss: 100.5623 - val_mae: 100.5623 - val_mse: 240338.9219\n",
      "TRAINING ENDED\n",
      "-------------------------------------------------------------------------------\n",
      "/n\n",
      "Strategy net profitability\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "-703598.5625473124\n",
      "-------------------------------------------------------------------------------\n",
      "/n\n",
      "the first few reults of our predictions - to check they arent identical\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "[[ 0.00039764]\n",
      " [ 0.00021408]\n",
      " [ 0.0008364 ]\n",
      " [-0.0000006 ]\n",
      " [ 0.00025471]\n",
      " [-0.00052522]\n",
      " [ 0.00002332]\n",
      " [ 0.000312  ]\n",
      " [ 0.0004785 ]\n",
      " [ 0.00020196]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_whole_model_assesment(build_simple_linear_mae())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 46800 samples, validate on 11700 samples\n",
      "Epoch 1/1000\n",
      "45408/46800 [============================>.] - ETA: 0s - loss: 1720034.9349 - mae: 123.7450 - mse: 1720037.6250\n",
      "Epoch: 0, loss:1675977.8628,  mae:123.5027,  mse:1675981.3750,  val_loss:237761.7018,  val_mae:111.6301,  val_mse:237761.5938,  \n",
      "46800/46800 [==============================] - 2s 38us/sample - loss: 1675977.8628 - mae: 123.5027 - mse: 1675981.3750 - val_loss: 237761.7018 - val_mae: 111.6301 - val_mse: 237761.5938\n",
      "Epoch 2/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1674608.2566 - mae: 129.0207 - mse: 1674609.0000 - val_loss: 236849.9198 - val_mae: 115.9577 - val_mse: 236849.9062\n",
      "Epoch 3/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1673630.3084 - mae: 133.1517 - mse: 1673631.3750 - val_loss: 236092.7030 - val_mae: 119.7873 - val_mse: 236092.7812\n",
      "Epoch 4/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1672669.2589 - mae: 137.1218 - mse: 1672669.1250 - val_loss: 235244.3809 - val_mae: 124.3808 - val_mse: 235244.5312\n",
      "Epoch 5/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1671788.9848 - mae: 141.2157 - mse: 1671789.3750 - val_loss: 234634.0717 - val_mae: 127.9071 - val_mse: 234633.9531\n",
      "Epoch 6/1000\n",
      "46800/46800 [==============================] - 2s 35us/sample - loss: 1671120.1806 - mae: 144.6084 - mse: 1671118.2500 - val_loss: 234139.7411 - val_mae: 130.9351 - val_mse: 234139.7031\n",
      "Epoch 7/1000\n",
      "46800/46800 [==============================] - 2s 36us/sample - loss: 1670515.0935 - mae: 147.6820 - mse: 1670514.3750 - val_loss: 233653.2476 - val_mae: 134.0969 - val_mse: 233653.2188\n",
      "Epoch 8/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1669911.9726 - mae: 150.8276 - mse: 1669910.8750 - val_loss: 233190.1791 - val_mae: 137.3040 - val_mse: 233190.1094\n",
      "Epoch 9/1000\n",
      "46800/46800 [==============================] - 2s 36us/sample - loss: 1669369.3307 - mae: 153.9279 - mse: 1669370.5000 - val_loss: 232779.5141 - val_mae: 140.3460 - val_mse: 232779.5781\n",
      "Epoch 10/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1668881.0801 - mae: 156.7827 - mse: 1668881.7500 - val_loss: 232435.1269 - val_mae: 143.0703 - val_mse: 232435.0938\n",
      "Epoch 11/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1668469.5745 - mae: 159.4225 - mse: 1668470.3750 - val_loss: 232133.1355 - val_mae: 145.6064 - val_mse: 232133.0469\n",
      "Epoch 12/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1668058.6442 - mae: 161.9743 - mse: 1668059.1250 - val_loss: 231838.2066 - val_mae: 148.2586 - val_mse: 231838.1875\n",
      "Epoch 13/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1667689.3372 - mae: 164.5233 - mse: 1667688.8750 - val_loss: 231577.6373 - val_mae: 150.7757 - val_mse: 231577.7031\n",
      "Epoch 14/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1667393.1677 - mae: 166.7911 - mse: 1667395.6250 - val_loss: 231383.8954 - val_mae: 152.7630 - val_mse: 231383.9375\n",
      "Epoch 15/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1667101.8505 - mae: 168.9072 - mse: 1667103.8750 - val_loss: 231182.5427 - val_mae: 154.9670 - val_mse: 231182.5312\n",
      "Epoch 16/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1666837.2554 - mae: 171.0363 - mse: 1666836.7500 - val_loss: 231011.0329 - val_mae: 156.9717 - val_mse: 231010.9688\n",
      "Epoch 17/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1666603.6886 - mae: 172.9329 - mse: 1666603.3750 - val_loss: 230861.2542 - val_mae: 158.8407 - val_mse: 230861.2344\n",
      "Epoch 18/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1666397.3704 - mae: 174.7730 - mse: 1666396.6250 - val_loss: 230728.6410 - val_mae: 160.6122 - val_mse: 230728.6719\n",
      "Epoch 19/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1666185.0464 - mae: 176.5241 - mse: 1666184.5000 - val_loss: 230594.8552 - val_mae: 162.5273 - val_mse: 230594.8281\n",
      "Epoch 20/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1666001.2070 - mae: 178.2071 - mse: 1666001.7500 - val_loss: 230489.9094 - val_mae: 164.1292 - val_mse: 230489.8438\n",
      "Epoch 21/1000\n",
      "46800/46800 [==============================] - 2s 35us/sample - loss: 1665849.3563 - mae: 179.8783 - mse: 1665847.8750 - val_loss: 230402.8718 - val_mae: 165.5583 - val_mse: 230402.9062\n",
      "Epoch 22/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1665700.6929 - mae: 181.2840 - mse: 1665700.2500 - val_loss: 230316.4406 - val_mae: 167.0576 - val_mse: 230316.2656\n",
      "Epoch 23/1000\n",
      "46800/46800 [==============================] - 2s 37us/sample - loss: 1665571.4737 - mae: 182.7252 - mse: 1665571.5000 - val_loss: 230246.0037 - val_mae: 168.3560 - val_mse: 230246.0000\n",
      "Epoch 24/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1665452.2897 - mae: 183.9237 - mse: 1665453.2500 - val_loss: 230177.7100 - val_mae: 169.7017 - val_mse: 230177.7188\n",
      "Epoch 25/1000\n",
      "46800/46800 [==============================] - 2s 35us/sample - loss: 1665332.5214 - mae: 185.1368 - mse: 1665332.6250 - val_loss: 230117.5480 - val_mae: 170.9645 - val_mse: 230117.4844\n",
      "Epoch 26/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1665222.1711 - mae: 186.4113 - mse: 1665220.8750 - val_loss: 230062.8711 - val_mae: 172.1938 - val_mse: 230063.0000\n",
      "Epoch 27/1000\n",
      "46800/46800 [==============================] - 2s 35us/sample - loss: 1665127.8071 - mae: 187.6096 - mse: 1665128.1250 - val_loss: 230018.0862 - val_mae: 173.2620 - val_mse: 230018.0625\n",
      "Epoch 28/1000\n",
      "46800/46800 [==============================] - 2s 39us/sample - loss: 1665044.6460 - mae: 188.5774 - mse: 1665045.1250 - val_loss: 229979.1498 - val_mae: 174.2400 - val_mse: 229979.1562\n",
      "Epoch 29/1000\n",
      "46800/46800 [==============================] - 2s 37us/sample - loss: 1664960.4436 - mae: 189.5311 - mse: 1664960.5000 - val_loss: 229936.6792 - val_mae: 175.3440 - val_mse: 229936.7344\n",
      "Epoch 30/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1664884.7483 - mae: 190.7049 - mse: 1664885.0000 - val_loss: 229906.7714 - val_mae: 176.1846 - val_mse: 229906.6875\n",
      "Epoch 31/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1664819.8153 - mae: 191.4599 - mse: 1664821.2500 - val_loss: 229878.4532 - val_mae: 177.0189 - val_mse: 229878.4844\n",
      "Epoch 32/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1664760.7969 - mae: 192.2605 - mse: 1664762.2500 - val_loss: 229852.3751 - val_mae: 177.8080 - val_mse: 229852.3125\n",
      "Epoch 33/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1664702.4075 - mae: 193.0790 - mse: 1664700.3750 - val_loss: 229828.9111 - val_mae: 178.5371 - val_mse: 229828.8750\n",
      "Epoch 34/1000\n",
      "46800/46800 [==============================] - 2s 38us/sample - loss: 1664647.2241 - mae: 193.8184 - mse: 1664648.7500 - val_loss: 229806.2651 - val_mae: 179.2855 - val_mse: 229806.2812\n",
      "Epoch 35/1000\n",
      "46800/46800 [==============================] - 2s 44us/sample - loss: 1664591.9418 - mae: 194.4632 - mse: 1664588.8750 - val_loss: 229784.4051 - val_mae: 180.0218 - val_mse: 229784.3750\n",
      "Epoch 36/1000\n",
      "46800/46800 [==============================] - 2s 42us/sample - loss: 1664539.5995 - mae: 195.1071 - mse: 1664539.2500 - val_loss: 229763.0712 - val_mae: 180.7442 - val_mse: 229763.0625\n",
      "Epoch 37/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1664491.5904 - mae: 195.7953 - mse: 1664492.2500 - val_loss: 229747.0461 - val_mae: 181.3018 - val_mse: 229747.1406\n",
      "Epoch 38/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1664453.7406 - mae: 196.4587 - mse: 1664452.6250 - val_loss: 229732.2109 - val_mae: 181.8159 - val_mse: 229732.0781\n",
      "Epoch 39/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1664414.8234 - mae: 196.9853 - mse: 1664414.5000 - val_loss: 229717.1037 - val_mae: 182.3394 - val_mse: 229717.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/1000\n",
      "46800/46800 [==============================] - 2s 35us/sample - loss: 1664372.5645 - mae: 197.3938 - mse: 1664372.1250 - val_loss: 229699.6597 - val_mae: 182.8970 - val_mse: 229699.5156\n",
      "Epoch 41/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1664331.8473 - mae: 197.9291 - mse: 1664331.1250 - val_loss: 229685.2738 - val_mae: 183.4075 - val_mse: 229685.2969\n",
      "Epoch 42/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1664295.9847 - mae: 198.4474 - mse: 1664296.1250 - val_loss: 229672.7197 - val_mae: 183.8494 - val_mse: 229672.7344\n",
      "Epoch 43/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1664262.8450 - mae: 198.9558 - mse: 1664261.5000 - val_loss: 229660.5530 - val_mae: 184.2679 - val_mse: 229660.7344\n",
      "Epoch 44/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1664232.0120 - mae: 199.2575 - mse: 1664233.0000 - val_loss: 229648.0837 - val_mae: 184.6789 - val_mse: 229648.0625\n",
      "Epoch 45/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1664197.4898 - mae: 199.6509 - mse: 1664197.1250 - val_loss: 229634.9263 - val_mae: 185.0758 - val_mse: 229634.9531\n",
      "Epoch 46/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1664167.1882 - mae: 200.1103 - mse: 1664166.8750 - val_loss: 229623.5613 - val_mae: 185.4020 - val_mse: 229623.5938\n",
      "Epoch 47/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1664137.5202 - mae: 200.4100 - mse: 1664138.6250 - val_loss: 229611.4974 - val_mae: 185.7756 - val_mse: 229611.6094\n",
      "Epoch 48/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1664108.7330 - mae: 200.7475 - mse: 1664108.8750 - val_loss: 229600.4951 - val_mae: 186.0628 - val_mse: 229600.4219\n",
      "Epoch 49/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1664081.9642 - mae: 200.9539 - mse: 1664081.8750 - val_loss: 229588.8923 - val_mae: 186.3535 - val_mse: 229588.8281\n",
      "Epoch 50/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1664054.5864 - mae: 201.3014 - mse: 1664053.3750 - val_loss: 229578.0468 - val_mae: 186.6466 - val_mse: 229578.0469\n",
      "Epoch 51/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1664028.1686 - mae: 201.5488 - mse: 1664028.1250 - val_loss: 229566.3021 - val_mae: 186.9189 - val_mse: 229566.2969\n",
      "Epoch 52/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1664003.1847 - mae: 201.8801 - mse: 1664001.6250 - val_loss: 229555.6905 - val_mae: 187.1438 - val_mse: 229555.6094\n",
      "Epoch 53/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1663978.6575 - mae: 202.0438 - mse: 1663979.7500 - val_loss: 229544.6722 - val_mae: 187.3707 - val_mse: 229544.7344\n",
      "Epoch 54/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1663952.8499 - mae: 202.2098 - mse: 1663953.2500 - val_loss: 229532.9997 - val_mae: 187.6354 - val_mse: 229532.9219\n",
      "Epoch 55/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1663927.2960 - mae: 202.4728 - mse: 1663926.5000 - val_loss: 229522.0276 - val_mae: 187.8559 - val_mse: 229522.0000\n",
      "Epoch 56/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1663903.9493 - mae: 202.7395 - mse: 1663902.6250 - val_loss: 229512.2343 - val_mae: 188.0587 - val_mse: 229512.1562\n",
      "Epoch 57/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1663882.1053 - mae: 202.9825 - mse: 1663882.5000 - val_loss: 229501.2836 - val_mae: 188.2397 - val_mse: 229501.3750\n",
      "Epoch 58/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1663859.4843 - mae: 203.1286 - mse: 1663859.6250 - val_loss: 229489.8581 - val_mae: 188.3980 - val_mse: 229489.9688\n",
      "Epoch 59/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1663837.0611 - mae: 203.3103 - mse: 1663836.5000 - val_loss: 229478.4936 - val_mae: 188.5475 - val_mse: 229478.3125\n",
      "Epoch 60/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1663814.9067 - mae: 203.4366 - mse: 1663814.6250 - val_loss: 229467.9573 - val_mae: 188.7305 - val_mse: 229467.9375\n",
      "Epoch 61/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1663793.4327 - mae: 203.6134 - mse: 1663793.2500 - val_loss: 229456.8955 - val_mae: 188.8623 - val_mse: 229456.9375\n",
      "Epoch 62/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1663771.7610 - mae: 203.6619 - mse: 1663771.8750 - val_loss: 229444.2279 - val_mae: 188.9829 - val_mse: 229444.2656\n",
      "Epoch 63/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1663750.0938 - mae: 203.8559 - mse: 1663750.0000 - val_loss: 229433.7994 - val_mae: 189.1185 - val_mse: 229433.8906\n",
      "Epoch 64/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1663729.5227 - mae: 203.9698 - mse: 1663728.1250 - val_loss: 229422.5588 - val_mae: 189.2437 - val_mse: 229422.6094\n",
      "Epoch 65/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1663708.6505 - mae: 204.2357 - mse: 1663709.7500 - val_loss: 229412.0394 - val_mae: 189.3605 - val_mse: 229412.1094\n",
      "Epoch 66/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1663688.1126 - mae: 204.1998 - mse: 1663687.3750 - val_loss: 229400.0261 - val_mae: 189.4751 - val_mse: 229400.0000\n",
      "Epoch 67/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1663667.9249 - mae: 204.4019 - mse: 1663670.8750 - val_loss: 229389.2135 - val_mae: 189.5627 - val_mse: 229389.2812\n",
      "Epoch 68/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1663648.1298 - mae: 204.4121 - mse: 1663648.0000 - val_loss: 229377.3077 - val_mae: 189.6534 - val_mse: 229377.2656\n",
      "Epoch 69/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1663627.7930 - mae: 204.4992 - mse: 1663627.8750 - val_loss: 229365.7601 - val_mae: 189.7460 - val_mse: 229365.7344\n",
      "Epoch 70/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1663607.7583 - mae: 204.6379 - mse: 1663606.6250 - val_loss: 229354.2365 - val_mae: 189.8351 - val_mse: 229354.2344\n",
      "Epoch 71/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1663588.7099 - mae: 204.7592 - mse: 1663588.1250 - val_loss: 229343.3741 - val_mae: 189.8914 - val_mse: 229343.4375\n",
      "Epoch 72/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1663570.0340 - mae: 204.6666 - mse: 1663568.7500 - val_loss: 229331.5750 - val_mae: 189.9676 - val_mse: 229331.4375\n",
      "Epoch 73/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1663551.0536 - mae: 204.8905 - mse: 1663551.0000 - val_loss: 229321.2161 - val_mae: 190.0494 - val_mse: 229321.2500\n",
      "Epoch 74/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1663532.1926 - mae: 204.9164 - mse: 1663531.3750 - val_loss: 229309.3324 - val_mae: 190.1119 - val_mse: 229309.2500\n",
      "Epoch 75/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1663513.6986 - mae: 204.9732 - mse: 1663515.0000 - val_loss: 229298.3114 - val_mae: 190.1775 - val_mse: 229298.2812\n",
      "Epoch 76/1000\n",
      "46800/46800 [==============================] - 2s 43us/sample - loss: 1663494.4514 - mae: 205.0446 - mse: 1663494.0000 - val_loss: 229287.4168 - val_mae: 190.2519 - val_mse: 229287.4219\n",
      "Epoch 77/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1663475.3953 - mae: 205.0251 - mse: 1663474.8750 - val_loss: 229275.0145 - val_mae: 190.3125 - val_mse: 229274.9531\n",
      "Epoch 78/1000\n",
      "46800/46800 [==============================] - 2s 36us/sample - loss: 1663455.8903 - mae: 205.2019 - mse: 1663457.2500 - val_loss: 229264.7903 - val_mae: 190.3760 - val_mse: 229264.7344\n",
      "Epoch 79/1000\n",
      "46800/46800 [==============================] - 2s 39us/sample - loss: 1663439.0940 - mae: 205.1918 - mse: 1663438.6250 - val_loss: 229254.2516 - val_mae: 190.4403 - val_mse: 229254.2812\n",
      "Epoch 80/1000\n",
      "46800/46800 [==============================] - 2s 41us/sample - loss: 1663421.3320 - mae: 205.2883 - mse: 1663421.0000 - val_loss: 229242.9168 - val_mae: 190.4636 - val_mse: 229242.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1663403.7560 - mae: 205.2291 - mse: 1663403.5000 - val_loss: 229231.1016 - val_mae: 190.5091 - val_mse: 229231.0938\n",
      "Epoch 82/1000\n",
      "46800/46800 [==============================] - 1s 29us/sample - loss: 1663385.7020 - mae: 205.4365 - mse: 1663383.6250 - val_loss: 229220.5209 - val_mae: 190.5261 - val_mse: 229220.5000\n",
      "Epoch 83/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1663369.4609 - mae: 205.4265 - mse: 1663370.5000 - val_loss: 229208.9794 - val_mae: 190.5483 - val_mse: 229208.9688\n",
      "Epoch 84/1000\n",
      "46800/46800 [==============================] - 1s 26us/sample - loss: 1663351.7925 - mae: 205.3486 - mse: 1663350.7500 - val_loss: 229198.4817 - val_mae: 190.6223 - val_mse: 229198.4688\n",
      "Epoch 85/1000\n",
      "46800/46800 [==============================] - 1s 26us/sample - loss: 1663333.1246 - mae: 205.5008 - mse: 1663334.1250 - val_loss: 229187.6611 - val_mae: 190.6750 - val_mse: 229187.7812\n",
      "Epoch 86/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1663317.0414 - mae: 205.5504 - mse: 1663316.5000 - val_loss: 229177.3618 - val_mae: 190.6964 - val_mse: 229177.4062\n",
      "Epoch 87/1000\n",
      "46800/46800 [==============================] - 1s 26us/sample - loss: 1663300.1255 - mae: 205.4999 - mse: 1663300.3750 - val_loss: 229165.2371 - val_mae: 190.7012 - val_mse: 229165.2344\n",
      "Epoch 88/1000\n",
      "46800/46800 [==============================] - 2s 35us/sample - loss: 1663282.7547 - mae: 205.4640 - mse: 1663282.6250 - val_loss: 229153.9783 - val_mae: 190.7295 - val_mse: 229153.9844\n",
      "Epoch 89/1000\n",
      "46800/46800 [==============================] - 1s 26us/sample - loss: 1663266.2602 - mae: 205.6009 - mse: 1663267.1250 - val_loss: 229144.4640 - val_mae: 190.7611 - val_mse: 229144.4375\n",
      "Epoch 90/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1663250.5372 - mae: 205.7042 - mse: 1663251.0000 - val_loss: 229134.5123 - val_mae: 190.7963 - val_mse: 229134.3906\n",
      "Epoch 91/1000\n",
      "46800/46800 [==============================] - 1s 25us/sample - loss: 1663232.6034 - mae: 205.6644 - mse: 1663232.5000 - val_loss: 229122.5930 - val_mae: 190.8397 - val_mse: 229122.5312\n",
      "Epoch 92/1000\n",
      "46800/46800 [==============================] - 1s 24us/sample - loss: 1663215.6271 - mae: 205.5822 - mse: 1663213.1250 - val_loss: 229110.8417 - val_mae: 190.8298 - val_mse: 229110.9375\n",
      "Epoch 93/1000\n",
      "46800/46800 [==============================] - 1s 27us/sample - loss: 1663199.5309 - mae: 205.6767 - mse: 1663197.7500 - val_loss: 229101.1339 - val_mae: 190.8515 - val_mse: 229101.1875\n",
      "Epoch 94/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1663183.7406 - mae: 205.7441 - mse: 1663185.1250 - val_loss: 229091.9803 - val_mae: 190.9042 - val_mse: 229091.9219\n",
      "Epoch 95/1000\n",
      "46800/46800 [==============================] - 2s 38us/sample - loss: 1663168.3799 - mae: 205.8036 - mse: 1663165.8750 - val_loss: 229082.0674 - val_mae: 190.9184 - val_mse: 229082.0781\n",
      "Epoch 96/1000\n",
      "46800/46800 [==============================] - 2s 35us/sample - loss: 1663152.2438 - mae: 205.6714 - mse: 1663152.1250 - val_loss: 229066.8359 - val_mae: 190.8436 - val_mse: 229066.7812\n",
      "Epoch 97/1000\n",
      "46800/46800 [==============================] - 1s 25us/sample - loss: 1663134.5755 - mae: 205.7379 - mse: 1663132.5000 - val_loss: 229059.4439 - val_mae: 190.9341 - val_mse: 229059.4531\n",
      "Epoch 98/1000\n",
      "46800/46800 [==============================] - 1s 23us/sample - loss: 1663119.6922 - mae: 205.8008 - mse: 1663121.2500 - val_loss: 229049.4142 - val_mae: 190.9587 - val_mse: 229049.4844\n",
      "Epoch 99/1000\n",
      "46800/46800 [==============================] - 1s 28us/sample - loss: 1663104.5105 - mae: 205.8191 - mse: 1663104.8750 - val_loss: 229039.8193 - val_mae: 190.9729 - val_mse: 229039.8594\n",
      "Epoch 100/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1663089.3267 - mae: 205.8802 - mse: 1663090.0000 - val_loss: 229029.8383 - val_mae: 190.9771 - val_mse: 229029.8125\n",
      "Epoch 101/1000\n",
      "45504/46800 [============================>.] - ETA: 0s - loss: 1701717.0187 - mae: 205.3573 - mse: 1701717.0000\n",
      "Epoch: 100, loss:1663073.2904,  mae:205.6640,  mse:1663073.3750,  val_loss:229017.6927,  val_mae:190.9902,  val_mse:229017.5938,  \n",
      "46800/46800 [==============================] - 1s 25us/sample - loss: 1663073.2904 - mae: 205.6640 - mse: 1663073.3750 - val_loss: 229017.6927 - val_mae: 190.9902 - val_mse: 229017.5938\n",
      "Epoch 102/1000\n",
      "46800/46800 [==============================] - 1s 25us/sample - loss: 1663056.6886 - mae: 205.8456 - mse: 1663055.2500 - val_loss: 229008.8028 - val_mae: 191.0054 - val_mse: 229008.9062\n",
      "Epoch 103/1000\n",
      "46800/46800 [==============================] - 1s 23us/sample - loss: 1663041.8216 - mae: 205.7760 - mse: 1663042.1250 - val_loss: 228997.1161 - val_mae: 190.9977 - val_mse: 228997.1562\n",
      "Epoch 104/1000\n",
      "46800/46800 [==============================] - 1s 23us/sample - loss: 1663026.0624 - mae: 205.8595 - mse: 1663024.6250 - val_loss: 228987.1067 - val_mae: 190.9822 - val_mse: 228987.1719\n",
      "Epoch 105/1000\n",
      "46800/46800 [==============================] - 1s 23us/sample - loss: 1663011.3526 - mae: 205.8184 - mse: 1663012.3750 - val_loss: 228978.7551 - val_mae: 191.0394 - val_mse: 228978.6562\n",
      "Epoch 106/1000\n",
      "46800/46800 [==============================] - 1s 24us/sample - loss: 1662997.0938 - mae: 205.8480 - mse: 1662995.2500 - val_loss: 228968.8317 - val_mae: 191.0258 - val_mse: 228968.8125\n",
      "Epoch 107/1000\n",
      "46800/46800 [==============================] - 1s 22us/sample - loss: 1662983.0234 - mae: 205.9620 - mse: 1662981.2500 - val_loss: 228960.1926 - val_mae: 191.0459 - val_mse: 228960.2500\n",
      "Epoch 108/1000\n",
      "46800/46800 [==============================] - 1s 22us/sample - loss: 1662968.3955 - mae: 205.7937 - mse: 1662968.0000 - val_loss: 228949.1583 - val_mae: 191.0501 - val_mse: 228949.1875\n",
      "Epoch 109/1000\n",
      "46800/46800 [==============================] - 1s 24us/sample - loss: 1662952.9651 - mae: 205.8897 - mse: 1662953.7500 - val_loss: 228939.6071 - val_mae: 191.0418 - val_mse: 228939.4219\n",
      "Epoch 110/1000\n",
      "46800/46800 [==============================] - 1s 23us/sample - loss: 1662939.2226 - mae: 205.8728 - mse: 1662939.2500 - val_loss: 228930.5603 - val_mae: 191.0440 - val_mse: 228930.3594\n",
      "Epoch 111/1000\n",
      "46800/46800 [==============================] - 1s 24us/sample - loss: 1662925.2825 - mae: 205.8815 - mse: 1662923.0000 - val_loss: 228921.3095 - val_mae: 191.0573 - val_mse: 228921.2812\n",
      "Epoch 112/1000\n",
      "46800/46800 [==============================] - 1s 23us/sample - loss: 1662910.9303 - mae: 205.8345 - mse: 1662910.5000 - val_loss: 228910.8666 - val_mae: 191.0494 - val_mse: 228910.7656\n",
      "Epoch 113/1000\n",
      "46800/46800 [==============================] - 1s 24us/sample - loss: 1662896.3565 - mae: 205.9258 - mse: 1662896.0000 - val_loss: 228902.1693 - val_mae: 191.0502 - val_mse: 228902.2188\n",
      "Epoch 114/1000\n",
      "46800/46800 [==============================] - 1s 23us/sample - loss: 1662882.8163 - mae: 205.8719 - mse: 1662883.8750 - val_loss: 228892.8427 - val_mae: 191.0533 - val_mse: 228892.7344\n",
      "Epoch 115/1000\n",
      "46800/46800 [==============================] - 1s 23us/sample - loss: 1662868.6495 - mae: 205.9160 - mse: 1662865.5000 - val_loss: 228884.4036 - val_mae: 191.0759 - val_mse: 228884.4219\n",
      "Epoch 116/1000\n",
      "46800/46800 [==============================] - 1s 23us/sample - loss: 1662855.0667 - mae: 205.8654 - mse: 1662855.2500 - val_loss: 228873.5985 - val_mae: 191.0520 - val_mse: 228873.6094\n",
      "Epoch 117/1000\n",
      "46800/46800 [==============================] - 1s 23us/sample - loss: 1662840.6231 - mae: 205.9111 - mse: 1662839.6250 - val_loss: 228865.1153 - val_mae: 191.0515 - val_mse: 228865.2500\n",
      "Epoch 118/1000\n",
      "46800/46800 [==============================] - 1s 28us/sample - loss: 1662827.4865 - mae: 206.0020 - mse: 1662829.5000 - val_loss: 228857.3157 - val_mae: 191.0696 - val_mse: 228857.2344\n",
      "Epoch 119/1000\n",
      "46800/46800 [==============================] - 1s 27us/sample - loss: 1662814.6726 - mae: 206.0407 - mse: 1662812.8750 - val_loss: 228847.9842 - val_mae: 191.0520 - val_mse: 228848.0469\n",
      "Epoch 120/1000\n",
      "46800/46800 [==============================] - 1s 26us/sample - loss: 1662801.0820 - mae: 206.0072 - mse: 1662801.2500 - val_loss: 228839.7128 - val_mae: 191.0775 - val_mse: 228839.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/1000\n",
      "46800/46800 [==============================] - 1s 26us/sample - loss: 1662786.8387 - mae: 205.9307 - mse: 1662787.8750 - val_loss: 228828.8104 - val_mae: 191.0546 - val_mse: 228828.9062\n",
      "Epoch 122/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1662772.9538 - mae: 205.9311 - mse: 1662772.2500 - val_loss: 228821.3441 - val_mae: 191.0709 - val_mse: 228821.4062\n",
      "Epoch 123/1000\n",
      "46800/46800 [==============================] - 2s 36us/sample - loss: 1662760.2176 - mae: 205.8784 - mse: 1662758.7500 - val_loss: 228812.3274 - val_mae: 191.0592 - val_mse: 228812.3125\n",
      "Epoch 124/1000\n",
      "46800/46800 [==============================] - 2s 36us/sample - loss: 1662746.9439 - mae: 205.8753 - mse: 1662747.3750 - val_loss: 228802.7207 - val_mae: 191.0464 - val_mse: 228802.7500\n",
      "Epoch 125/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662733.7356 - mae: 205.8886 - mse: 1662732.8750 - val_loss: 228794.5092 - val_mae: 191.0395 - val_mse: 228794.3750\n",
      "Epoch 126/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1662720.9708 - mae: 205.8250 - mse: 1662722.7500 - val_loss: 228786.2859 - val_mae: 191.0469 - val_mse: 228786.2969\n",
      "Epoch 127/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1662707.8486 - mae: 205.8847 - mse: 1662707.6250 - val_loss: 228778.3580 - val_mae: 191.0658 - val_mse: 228778.3750\n",
      "Epoch 128/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662694.8512 - mae: 205.9547 - mse: 1662695.8750 - val_loss: 228770.3200 - val_mae: 191.0677 - val_mse: 228770.2812\n",
      "Epoch 129/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1662682.4733 - mae: 205.9102 - mse: 1662678.3750 - val_loss: 228762.7137 - val_mae: 191.0795 - val_mse: 228762.6562\n",
      "Epoch 130/1000\n",
      "46800/46800 [==============================] - 2s 37us/sample - loss: 1662670.2688 - mae: 205.9139 - mse: 1662669.3750 - val_loss: 228754.0586 - val_mae: 191.0632 - val_mse: 228753.9844\n",
      "Epoch 131/1000\n",
      "46800/46800 [==============================] - 2s 39us/sample - loss: 1662657.9055 - mae: 205.9180 - mse: 1662659.8750 - val_loss: 228746.0834 - val_mae: 191.0588 - val_mse: 228746.0000\n",
      "Epoch 132/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1662644.8542 - mae: 205.9674 - mse: 1662644.8750 - val_loss: 228739.0384 - val_mae: 191.0926 - val_mse: 228739.0781\n",
      "Epoch 133/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662632.9824 - mae: 205.9907 - mse: 1662633.1250 - val_loss: 228731.3055 - val_mae: 191.0889 - val_mse: 228731.2969\n",
      "Epoch 134/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662621.0135 - mae: 205.9254 - mse: 1662620.1250 - val_loss: 228723.4589 - val_mae: 191.0900 - val_mse: 228723.3906\n",
      "Epoch 135/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1662608.1851 - mae: 205.9505 - mse: 1662608.2500 - val_loss: 228715.5397 - val_mae: 191.0995 - val_mse: 228715.6094\n",
      "Epoch 136/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662595.9873 - mae: 205.8887 - mse: 1662596.6250 - val_loss: 228706.5660 - val_mae: 191.0633 - val_mse: 228706.4375\n",
      "Epoch 137/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662583.7484 - mae: 205.9928 - mse: 1662584.6250 - val_loss: 228700.4462 - val_mae: 191.0991 - val_mse: 228700.4688\n",
      "Epoch 138/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662572.3953 - mae: 205.9816 - mse: 1662575.8750 - val_loss: 228692.4143 - val_mae: 191.0762 - val_mse: 228692.4844\n",
      "Epoch 139/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1662560.3178 - mae: 206.0218 - mse: 1662559.3750 - val_loss: 228684.6047 - val_mae: 191.0801 - val_mse: 228684.6406\n",
      "Epoch 140/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1662548.3108 - mae: 205.9452 - mse: 1662549.1250 - val_loss: 228676.7745 - val_mae: 191.0727 - val_mse: 228676.7656\n",
      "Epoch 141/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662536.4393 - mae: 205.9645 - mse: 1662536.0000 - val_loss: 228669.9642 - val_mae: 191.0844 - val_mse: 228669.9375\n",
      "Epoch 142/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662524.8908 - mae: 205.8587 - mse: 1662524.2500 - val_loss: 228661.4929 - val_mae: 191.0646 - val_mse: 228661.5156\n",
      "Epoch 143/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662512.9792 - mae: 205.8658 - mse: 1662512.8750 - val_loss: 228653.1824 - val_mae: 191.0330 - val_mse: 228653.1250\n",
      "Epoch 144/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1662501.5156 - mae: 205.8587 - mse: 1662501.5000 - val_loss: 228645.2844 - val_mae: 191.0242 - val_mse: 228645.2188\n",
      "Epoch 145/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662489.9481 - mae: 205.8672 - mse: 1662489.2500 - val_loss: 228638.5589 - val_mae: 191.0280 - val_mse: 228638.4062\n",
      "Epoch 146/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662478.9117 - mae: 205.9474 - mse: 1662482.5000 - val_loss: 228632.4206 - val_mae: 191.0377 - val_mse: 228632.3281\n",
      "Epoch 147/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662468.2483 - mae: 205.9179 - mse: 1662471.1250 - val_loss: 228625.0317 - val_mae: 191.0227 - val_mse: 228625.0938\n",
      "Epoch 148/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662456.7528 - mae: 205.9688 - mse: 1662458.3750 - val_loss: 228617.8587 - val_mae: 191.0172 - val_mse: 228617.6875\n",
      "Epoch 149/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662445.6909 - mae: 205.8534 - mse: 1662446.1250 - val_loss: 228610.2831 - val_mae: 191.0054 - val_mse: 228610.3906\n",
      "Epoch 150/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1662434.0341 - mae: 205.9230 - mse: 1662432.6250 - val_loss: 228604.5249 - val_mae: 191.0404 - val_mse: 228604.5000\n",
      "Epoch 151/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1662423.2120 - mae: 205.8942 - mse: 1662423.0000 - val_loss: 228594.5671 - val_mae: 190.9621 - val_mse: 228594.4844\n",
      "Epoch 152/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662411.3417 - mae: 205.9257 - mse: 1662412.0000 - val_loss: 228589.7652 - val_mae: 191.0162 - val_mse: 228589.7500\n",
      "Epoch 153/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662401.0641 - mae: 205.8033 - mse: 1662400.1250 - val_loss: 228581.5771 - val_mae: 190.9979 - val_mse: 228581.5938\n",
      "Epoch 154/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662388.8642 - mae: 205.8519 - mse: 1662388.7500 - val_loss: 228575.8717 - val_mae: 191.0197 - val_mse: 228575.8750\n",
      "Epoch 155/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1662378.9839 - mae: 205.8319 - mse: 1662377.5000 - val_loss: 228569.1123 - val_mae: 191.0072 - val_mse: 228569.0469\n",
      "Epoch 156/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662368.2211 - mae: 205.8853 - mse: 1662368.3750 - val_loss: 228562.9591 - val_mae: 191.0226 - val_mse: 228562.9688\n",
      "Epoch 157/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662357.2145 - mae: 205.9475 - mse: 1662353.5000 - val_loss: 228556.8577 - val_mae: 191.0194 - val_mse: 228556.8438\n",
      "Epoch 158/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662347.5523 - mae: 206.0376 - mse: 1662346.5000 - val_loss: 228550.6094 - val_mae: 191.0081 - val_mse: 228550.6719\n",
      "Epoch 159/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1662337.7873 - mae: 205.8404 - mse: 1662336.0000 - val_loss: 228543.6518 - val_mae: 191.0046 - val_mse: 228543.6875\n",
      "Epoch 160/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1662326.0851 - mae: 205.9301 - mse: 1662327.6250 - val_loss: 228537.6495 - val_mae: 191.0201 - val_mse: 228537.7656\n",
      "Epoch 161/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1662316.2903 - mae: 205.9909 - mse: 1662316.1250 - val_loss: 228531.8334 - val_mae: 191.0242 - val_mse: 228531.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1662306.0863 - mae: 205.8371 - mse: 1662306.5000 - val_loss: 228523.4174 - val_mae: 190.9950 - val_mse: 228523.3438\n",
      "Epoch 163/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1662294.3398 - mae: 205.8049 - mse: 1662293.3750 - val_loss: 228516.5326 - val_mae: 190.9814 - val_mse: 228516.6250\n",
      "Epoch 164/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662284.3256 - mae: 205.8150 - mse: 1662285.5000 - val_loss: 228510.7324 - val_mae: 190.9777 - val_mse: 228510.7812\n",
      "Epoch 165/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662274.2754 - mae: 205.8581 - mse: 1662273.2500 - val_loss: 228504.9074 - val_mae: 190.9778 - val_mse: 228505.0312\n",
      "Epoch 166/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662264.5216 - mae: 205.9835 - mse: 1662264.1250 - val_loss: 228498.8713 - val_mae: 190.9572 - val_mse: 228498.9062\n",
      "Epoch 167/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662255.6785 - mae: 205.8464 - mse: 1662254.5000 - val_loss: 228492.6590 - val_mae: 190.9494 - val_mse: 228492.7344\n",
      "Epoch 168/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662245.8265 - mae: 205.8555 - mse: 1662245.2500 - val_loss: 228486.4170 - val_mae: 190.9385 - val_mse: 228486.4375\n",
      "Epoch 169/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1662235.6652 - mae: 205.7636 - mse: 1662234.7500 - val_loss: 228480.5255 - val_mae: 190.9608 - val_mse: 228480.3906\n",
      "Epoch 170/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1662224.8519 - mae: 205.8584 - mse: 1662227.0000 - val_loss: 228474.8708 - val_mae: 190.9626 - val_mse: 228474.7656\n",
      "Epoch 171/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662215.3390 - mae: 205.8265 - mse: 1662216.1250 - val_loss: 228468.8902 - val_mae: 190.9616 - val_mse: 228468.8906\n",
      "Epoch 172/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1662205.2988 - mae: 205.8557 - mse: 1662204.7500 - val_loss: 228462.8993 - val_mae: 190.9591 - val_mse: 228462.9531\n",
      "Epoch 173/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662195.5926 - mae: 205.9288 - mse: 1662195.5000 - val_loss: 228457.8549 - val_mae: 190.9653 - val_mse: 228457.8281\n",
      "Epoch 174/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662187.1429 - mae: 205.7529 - mse: 1662188.2500 - val_loss: 228450.3765 - val_mae: 190.9375 - val_mse: 228450.5312\n",
      "Epoch 175/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662176.4149 - mae: 205.7926 - mse: 1662175.8750 - val_loss: 228444.6255 - val_mae: 190.9293 - val_mse: 228444.5312\n",
      "Epoch 176/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662166.8709 - mae: 205.8184 - mse: 1662168.1250 - val_loss: 228439.6126 - val_mae: 190.9329 - val_mse: 228439.5938\n",
      "Epoch 177/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662157.9596 - mae: 205.7758 - mse: 1662157.8750 - val_loss: 228433.5702 - val_mae: 190.9312 - val_mse: 228433.5938\n",
      "Epoch 178/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662148.1198 - mae: 205.8511 - mse: 1662148.2500 - val_loss: 228429.1093 - val_mae: 190.9458 - val_mse: 228429.1719\n",
      "Epoch 179/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662139.3174 - mae: 205.8067 - mse: 1662138.2500 - val_loss: 228423.3755 - val_mae: 190.9388 - val_mse: 228423.2500\n",
      "Epoch 180/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1662129.9348 - mae: 205.8762 - mse: 1662128.5000 - val_loss: 228418.4394 - val_mae: 190.9453 - val_mse: 228418.3906\n",
      "Epoch 181/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1662120.7320 - mae: 205.9147 - mse: 1662121.2500 - val_loss: 228413.2987 - val_mae: 190.9513 - val_mse: 228413.3281\n",
      "Epoch 182/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662112.3645 - mae: 205.7445 - mse: 1662111.8750 - val_loss: 228406.5150 - val_mae: 190.9327 - val_mse: 228406.4375\n",
      "Epoch 183/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662101.6178 - mae: 205.8539 - mse: 1662101.0000 - val_loss: 228402.2962 - val_mae: 190.9562 - val_mse: 228402.3750\n",
      "Epoch 184/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662092.9816 - mae: 205.8628 - mse: 1662093.3750 - val_loss: 228396.8846 - val_mae: 190.9387 - val_mse: 228396.9062\n",
      "Epoch 185/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1662084.2953 - mae: 205.8161 - mse: 1662085.5000 - val_loss: 228392.3296 - val_mae: 190.9584 - val_mse: 228392.3438\n",
      "Epoch 186/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1662075.3531 - mae: 205.8846 - mse: 1662074.8750 - val_loss: 228386.6977 - val_mae: 190.9327 - val_mse: 228386.7656\n",
      "Epoch 187/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1662066.8087 - mae: 205.8572 - mse: 1662066.6250 - val_loss: 228381.9691 - val_mae: 190.9431 - val_mse: 228381.9531\n",
      "Epoch 188/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1662058.0860 - mae: 205.8755 - mse: 1662057.3750 - val_loss: 228377.1448 - val_mae: 190.9536 - val_mse: 228377.2344\n",
      "Epoch 189/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662049.4204 - mae: 205.8821 - mse: 1662049.8750 - val_loss: 228372.0280 - val_mae: 190.9494 - val_mse: 228371.9844\n",
      "Epoch 190/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662040.3639 - mae: 205.8252 - mse: 1662039.8750 - val_loss: 228366.3956 - val_mae: 190.9464 - val_mse: 228366.4844\n",
      "Epoch 191/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1662031.1789 - mae: 205.8468 - mse: 1662033.0000 - val_loss: 228362.2012 - val_mae: 190.9660 - val_mse: 228362.3125\n",
      "Epoch 192/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662022.7835 - mae: 205.9057 - mse: 1662022.3750 - val_loss: 228357.6332 - val_mae: 190.9722 - val_mse: 228357.6719\n",
      "Epoch 193/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1662014.6450 - mae: 205.8250 - mse: 1662015.5000 - val_loss: 228352.3800 - val_mae: 190.9605 - val_mse: 228352.3906\n",
      "Epoch 194/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1662005.5761 - mae: 205.8270 - mse: 1662004.7500 - val_loss: 228347.1478 - val_mae: 190.9654 - val_mse: 228347.0781\n",
      "Epoch 195/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661996.4235 - mae: 205.8985 - mse: 1662000.1250 - val_loss: 228342.7826 - val_mae: 190.9608 - val_mse: 228342.8750\n",
      "Epoch 196/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661988.9785 - mae: 205.7314 - mse: 1661988.7500 - val_loss: 228336.3167 - val_mae: 190.9339 - val_mse: 228336.2500\n",
      "Epoch 197/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661979.2347 - mae: 205.8545 - mse: 1661979.2500 - val_loss: 228333.7436 - val_mae: 190.9952 - val_mse: 228333.7812\n",
      "Epoch 198/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661971.3933 - mae: 205.8750 - mse: 1661970.2500 - val_loss: 228328.6208 - val_mae: 190.9795 - val_mse: 228328.7344\n",
      "Epoch 199/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661963.0252 - mae: 205.9487 - mse: 1661963.0000 - val_loss: 228325.0403 - val_mae: 191.0022 - val_mse: 228325.0000\n",
      "Epoch 200/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661955.6511 - mae: 205.9137 - mse: 1661955.3750 - val_loss: 228320.3090 - val_mae: 190.9940 - val_mse: 228320.2344\n",
      "Epoch 201/1000\n",
      "45216/46800 [===========================>..] - ETA: 0s - loss: 1710023.1045 - mae: 206.1992 - mse: 1710022.8750\n",
      "Epoch: 200, loss:1661947.2048,  mae:205.8345,  mse:1661946.2500,  val_loss:228315.5197,  val_mae:191.0071,  val_mse:228315.4531,  \n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661947.2048 - mae: 205.8345 - mse: 1661946.2500 - val_loss: 228315.5197 - val_mae: 191.0071 - val_mse: 228315.4531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661938.4630 - mae: 205.8576 - mse: 1661938.7500 - val_loss: 228310.6832 - val_mae: 190.9969 - val_mse: 228310.7188\n",
      "Epoch 203/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661930.0070 - mae: 205.9183 - mse: 1661930.2500 - val_loss: 228306.7181 - val_mae: 191.0045 - val_mse: 228306.6562\n",
      "Epoch 204/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661922.8658 - mae: 205.8752 - mse: 1661921.8750 - val_loss: 228301.9862 - val_mae: 191.0014 - val_mse: 228302.0469\n",
      "Epoch 205/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661914.6750 - mae: 205.9167 - mse: 1661915.2500 - val_loss: 228297.5177 - val_mae: 190.9896 - val_mse: 228297.5469\n",
      "Epoch 206/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661907.0466 - mae: 205.8125 - mse: 1661906.8750 - val_loss: 228293.4648 - val_mae: 191.0109 - val_mse: 228293.5312\n",
      "Epoch 207/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661898.6607 - mae: 205.9064 - mse: 1661898.2500 - val_loss: 228288.8928 - val_mae: 191.0078 - val_mse: 228288.9219\n",
      "Epoch 208/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661890.6922 - mae: 205.9345 - mse: 1661888.7500 - val_loss: 228284.7282 - val_mae: 191.0055 - val_mse: 228284.7344\n",
      "Epoch 209/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661883.1156 - mae: 205.8577 - mse: 1661884.1250 - val_loss: 228280.0797 - val_mae: 190.9962 - val_mse: 228280.0938\n",
      "Epoch 210/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661875.1697 - mae: 205.9080 - mse: 1661877.1250 - val_loss: 228276.7366 - val_mae: 191.0218 - val_mse: 228276.7812\n",
      "Epoch 211/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661867.4908 - mae: 205.9475 - mse: 1661867.5000 - val_loss: 228272.3632 - val_mae: 191.0161 - val_mse: 228272.3125\n",
      "Epoch 212/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661859.7358 - mae: 205.9680 - mse: 1661859.2500 - val_loss: 228268.4445 - val_mae: 191.0275 - val_mse: 228268.4375\n",
      "Epoch 213/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661851.6248 - mae: 205.9940 - mse: 1661850.8750 - val_loss: 228264.7800 - val_mae: 191.0484 - val_mse: 228264.6719\n",
      "Epoch 214/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661844.2673 - mae: 206.0123 - mse: 1661845.7500 - val_loss: 228259.6357 - val_mae: 191.0204 - val_mse: 228259.7031\n",
      "Epoch 215/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661837.3361 - mae: 205.7813 - mse: 1661837.7500 - val_loss: 228253.6652 - val_mae: 190.9720 - val_mse: 228253.8281\n",
      "Epoch 216/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661829.1965 - mae: 205.8428 - mse: 1661829.5000 - val_loss: 228250.1913 - val_mae: 190.9890 - val_mse: 228250.2969\n",
      "Epoch 217/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661821.5584 - mae: 205.9468 - mse: 1661822.5000 - val_loss: 228246.4062 - val_mae: 190.9884 - val_mse: 228246.4688\n",
      "Epoch 218/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661814.1491 - mae: 205.8023 - mse: 1661813.7500 - val_loss: 228242.2547 - val_mae: 190.9948 - val_mse: 228242.2188\n",
      "Epoch 219/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661806.4246 - mae: 205.9183 - mse: 1661807.6250 - val_loss: 228238.6856 - val_mae: 190.9927 - val_mse: 228238.6562\n",
      "Epoch 220/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661800.1964 - mae: 205.9200 - mse: 1661801.6250 - val_loss: 228235.1915 - val_mae: 191.0065 - val_mse: 228235.1406\n",
      "Epoch 221/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661792.6459 - mae: 205.9039 - mse: 1661792.7500 - val_loss: 228231.8115 - val_mae: 191.0256 - val_mse: 228231.8594\n",
      "Epoch 222/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661784.9461 - mae: 206.0119 - mse: 1661784.2500 - val_loss: 228228.5031 - val_mae: 191.0413 - val_mse: 228228.5000\n",
      "Epoch 223/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661777.7651 - mae: 205.9439 - mse: 1661777.6250 - val_loss: 228224.3814 - val_mae: 191.0419 - val_mse: 228224.3750\n",
      "Epoch 224/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661770.5807 - mae: 205.8571 - mse: 1661770.5000 - val_loss: 228220.1626 - val_mae: 191.0403 - val_mse: 228220.0625\n",
      "Epoch 225/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661762.4524 - mae: 205.9873 - mse: 1661761.7500 - val_loss: 228217.0027 - val_mae: 191.0517 - val_mse: 228216.9375\n",
      "Epoch 226/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661755.8537 - mae: 205.9294 - mse: 1661755.6250 - val_loss: 228213.1244 - val_mae: 191.0583 - val_mse: 228213.1250\n",
      "Epoch 227/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661748.5821 - mae: 205.8914 - mse: 1661749.6250 - val_loss: 228209.2300 - val_mae: 191.0564 - val_mse: 228209.2344\n",
      "Epoch 228/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661741.5037 - mae: 206.0196 - mse: 1661742.0000 - val_loss: 228205.9745 - val_mae: 191.0577 - val_mse: 228205.8906\n",
      "Epoch 229/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661735.2114 - mae: 205.9524 - mse: 1661736.8750 - val_loss: 228202.1301 - val_mae: 191.0525 - val_mse: 228202.0938\n",
      "Epoch 230/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661728.1159 - mae: 205.8802 - mse: 1661728.5000 - val_loss: 228198.9161 - val_mae: 191.0831 - val_mse: 228198.9062\n",
      "Epoch 231/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661720.7805 - mae: 205.8327 - mse: 1661721.2500 - val_loss: 228194.3007 - val_mae: 191.0560 - val_mse: 228194.3750\n",
      "Epoch 232/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1661713.6588 - mae: 205.9546 - mse: 1661711.8750 - val_loss: 228191.1418 - val_mae: 191.0642 - val_mse: 228191.1719\n",
      "Epoch 233/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661706.8700 - mae: 205.8566 - mse: 1661706.7500 - val_loss: 228187.0147 - val_mae: 191.0602 - val_mse: 228186.9844\n",
      "Epoch 234/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661699.4311 - mae: 205.9677 - mse: 1661698.2500 - val_loss: 228184.8169 - val_mae: 191.0909 - val_mse: 228184.7500\n",
      "Epoch 235/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661692.9802 - mae: 206.0020 - mse: 1661693.5000 - val_loss: 228181.4262 - val_mae: 191.0957 - val_mse: 228181.4219\n",
      "Epoch 236/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1661686.6248 - mae: 206.0318 - mse: 1661685.7500 - val_loss: 228177.4392 - val_mae: 191.0785 - val_mse: 228177.3750\n",
      "Epoch 237/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661680.0060 - mae: 206.0521 - mse: 1661680.0000 - val_loss: 228174.2173 - val_mae: 191.0868 - val_mse: 228174.3281\n",
      "Epoch 238/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1661673.4057 - mae: 205.9506 - mse: 1661674.3750 - val_loss: 228170.8606 - val_mae: 191.0889 - val_mse: 228170.8750\n",
      "Epoch 239/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661666.3846 - mae: 206.0970 - mse: 1661669.1250 - val_loss: 228167.6126 - val_mae: 191.0854 - val_mse: 228167.6875\n",
      "Epoch 240/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661660.7315 - mae: 206.0829 - mse: 1661662.2500 - val_loss: 228164.0683 - val_mae: 191.0786 - val_mse: 228164.1406\n",
      "Epoch 241/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661654.0134 - mae: 206.1003 - mse: 1661651.5000 - val_loss: 228160.6835 - val_mae: 191.0766 - val_mse: 228160.6719\n",
      "Epoch 242/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661648.2891 - mae: 205.9165 - mse: 1661648.8750 - val_loss: 228155.9952 - val_mae: 191.0502 - val_mse: 228156.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661641.1417 - mae: 205.8825 - mse: 1661641.0000 - val_loss: 228150.6723 - val_mae: 190.9966 - val_mse: 228150.6562\n",
      "Epoch 244/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661634.1212 - mae: 206.0064 - mse: 1661637.1250 - val_loss: 228149.9876 - val_mae: 191.0697 - val_mse: 228150.0156\n",
      "Epoch 245/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661627.9078 - mae: 206.0409 - mse: 1661628.3750 - val_loss: 228147.6709 - val_mae: 191.0946 - val_mse: 228147.7500\n",
      "Epoch 246/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661622.3757 - mae: 205.9696 - mse: 1661621.0000 - val_loss: 228143.2748 - val_mae: 191.0586 - val_mse: 228143.2812\n",
      "Epoch 247/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661615.1027 - mae: 206.1059 - mse: 1661613.2500 - val_loss: 228140.9226 - val_mae: 191.0827 - val_mse: 228140.9375\n",
      "Epoch 248/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661609.3583 - mae: 205.9727 - mse: 1661608.6250 - val_loss: 228138.2548 - val_mae: 191.1005 - val_mse: 228138.2500\n",
      "Epoch 249/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661602.6956 - mae: 206.0571 - mse: 1661603.0000 - val_loss: 228135.6663 - val_mae: 191.1216 - val_mse: 228135.7500\n",
      "Epoch 250/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661596.9851 - mae: 205.9175 - mse: 1661596.6250 - val_loss: 228129.2374 - val_mae: 191.0339 - val_mse: 228129.2656\n",
      "Epoch 251/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661589.5399 - mae: 205.9879 - mse: 1661589.8750 - val_loss: 228129.3125 - val_mae: 191.1283 - val_mse: 228129.3750\n",
      "Epoch 252/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661583.7927 - mae: 206.0285 - mse: 1661581.3750 - val_loss: 228126.8581 - val_mae: 191.1427 - val_mse: 228126.8906\n",
      "Epoch 253/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661577.6572 - mae: 206.1243 - mse: 1661579.6250 - val_loss: 228123.9621 - val_mae: 191.1413 - val_mse: 228123.9219\n",
      "Epoch 254/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661571.6484 - mae: 206.0923 - mse: 1661570.8750 - val_loss: 228121.3249 - val_mae: 191.1588 - val_mse: 228121.3438\n",
      "Epoch 255/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661565.6814 - mae: 206.1105 - mse: 1661568.6250 - val_loss: 228118.4288 - val_mae: 191.1600 - val_mse: 228118.4688\n",
      "Epoch 256/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661559.8513 - mae: 206.0987 - mse: 1661556.7500 - val_loss: 228115.3602 - val_mae: 191.1619 - val_mse: 228115.4531\n",
      "Epoch 257/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661553.9600 - mae: 206.0980 - mse: 1661554.8750 - val_loss: 228112.1592 - val_mae: 191.1484 - val_mse: 228112.1094\n",
      "Epoch 258/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661548.3292 - mae: 206.0613 - mse: 1661550.5000 - val_loss: 228109.4380 - val_mae: 191.1547 - val_mse: 228109.4062\n",
      "Epoch 259/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661542.1699 - mae: 206.1025 - mse: 1661542.2500 - val_loss: 228107.2687 - val_mae: 191.1783 - val_mse: 228107.2500\n",
      "Epoch 260/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661535.8827 - mae: 206.0823 - mse: 1661535.0000 - val_loss: 228104.9613 - val_mae: 191.2070 - val_mse: 228104.9688\n",
      "Epoch 261/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661529.4921 - mae: 206.1497 - mse: 1661529.5000 - val_loss: 228102.3065 - val_mae: 191.2126 - val_mse: 228102.3438\n",
      "Epoch 262/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661524.2337 - mae: 206.0984 - mse: 1661522.6250 - val_loss: 228098.9943 - val_mae: 191.2003 - val_mse: 228099.0469\n",
      "Epoch 263/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661518.4440 - mae: 206.0635 - mse: 1661518.1250 - val_loss: 228096.3952 - val_mae: 191.2096 - val_mse: 228096.5469\n",
      "Epoch 264/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661512.1562 - mae: 206.0615 - mse: 1661512.0000 - val_loss: 228093.5488 - val_mae: 191.2194 - val_mse: 228093.5000\n",
      "Epoch 265/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661505.9092 - mae: 206.1798 - mse: 1661505.5000 - val_loss: 228090.8247 - val_mae: 191.2101 - val_mse: 228090.7656\n",
      "Epoch 266/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661500.4547 - mae: 206.2071 - mse: 1661501.2500 - val_loss: 228088.9454 - val_mae: 191.2395 - val_mse: 228088.9062\n",
      "Epoch 267/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661494.8249 - mae: 206.1098 - mse: 1661494.5000 - val_loss: 228086.2988 - val_mae: 191.2522 - val_mse: 228086.3281\n",
      "Epoch 268/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661488.5775 - mae: 206.2348 - mse: 1661488.2500 - val_loss: 228083.1113 - val_mae: 191.2288 - val_mse: 228083.1094\n",
      "Epoch 269/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661483.8570 - mae: 206.1595 - mse: 1661483.2500 - val_loss: 228080.9560 - val_mae: 191.2444 - val_mse: 228080.9062\n",
      "Epoch 270/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661478.4253 - mae: 206.0424 - mse: 1661477.6250 - val_loss: 228077.2940 - val_mae: 191.2295 - val_mse: 228077.3438\n",
      "Epoch 271/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661471.8381 - mae: 206.2037 - mse: 1661472.5000 - val_loss: 228074.9315 - val_mae: 191.2298 - val_mse: 228075.0000\n",
      "Epoch 272/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661467.4589 - mae: 206.1920 - mse: 1661467.6250 - val_loss: 228070.6518 - val_mae: 191.1861 - val_mse: 228070.7656\n",
      "Epoch 273/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661461.4179 - mae: 206.0763 - mse: 1661461.7500 - val_loss: 228068.4752 - val_mae: 191.1999 - val_mse: 228068.5000\n",
      "Epoch 274/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661455.9902 - mae: 206.0518 - mse: 1661456.1250 - val_loss: 228066.1741 - val_mae: 191.2153 - val_mse: 228066.1562\n",
      "Epoch 275/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661450.2377 - mae: 206.1162 - mse: 1661449.6250 - val_loss: 228064.2486 - val_mae: 191.2285 - val_mse: 228064.2969\n",
      "Epoch 276/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661444.9781 - mae: 206.1595 - mse: 1661446.0000 - val_loss: 228062.3135 - val_mae: 191.2417 - val_mse: 228062.2812\n",
      "Epoch 277/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661439.5019 - mae: 206.2537 - mse: 1661440.8750 - val_loss: 228060.5017 - val_mae: 191.2606 - val_mse: 228060.4062\n",
      "Epoch 278/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661435.5658 - mae: 206.0825 - mse: 1661435.5000 - val_loss: 228057.0565 - val_mae: 191.2382 - val_mse: 228056.9688\n",
      "Epoch 279/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661429.2263 - mae: 206.1462 - mse: 1661428.2500 - val_loss: 228055.3032 - val_mae: 191.2598 - val_mse: 228055.3438\n",
      "Epoch 280/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661423.5691 - mae: 206.1893 - mse: 1661424.3750 - val_loss: 228053.6763 - val_mae: 191.2905 - val_mse: 228053.7500\n",
      "Epoch 281/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661418.0512 - mae: 206.2720 - mse: 1661420.3750 - val_loss: 228051.7730 - val_mae: 191.3039 - val_mse: 228051.5625\n",
      "Epoch 282/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661413.4783 - mae: 206.1270 - mse: 1661413.2500 - val_loss: 228049.2589 - val_mae: 191.3144 - val_mse: 228049.3281\n",
      "Epoch 283/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661407.2999 - mae: 206.2998 - mse: 1661408.5000 - val_loss: 228047.6837 - val_mae: 191.3335 - val_mse: 228047.6719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661402.2288 - mae: 206.2994 - mse: 1661404.2500 - val_loss: 228045.2912 - val_mae: 191.3363 - val_mse: 228045.4062\n",
      "Epoch 285/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661397.0313 - mae: 206.2538 - mse: 1661398.3750 - val_loss: 228043.2600 - val_mae: 191.3481 - val_mse: 228043.2031\n",
      "Epoch 286/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661391.9616 - mae: 206.2359 - mse: 1661392.0000 - val_loss: 228040.7079 - val_mae: 191.3428 - val_mse: 228040.6719\n",
      "Epoch 287/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661386.9645 - mae: 206.1447 - mse: 1661387.8750 - val_loss: 228036.2289 - val_mae: 191.2766 - val_mse: 228036.2656\n",
      "Epoch 288/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661381.7665 - mae: 206.2744 - mse: 1661383.5000 - val_loss: 228036.2365 - val_mae: 191.3429 - val_mse: 228036.1719\n",
      "Epoch 289/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661377.8314 - mae: 206.2285 - mse: 1661378.2500 - val_loss: 228032.9234 - val_mae: 191.3083 - val_mse: 228032.7500\n",
      "Epoch 290/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661372.4317 - mae: 206.2726 - mse: 1661370.6250 - val_loss: 228030.9665 - val_mae: 191.3161 - val_mse: 228031.0000\n",
      "Epoch 291/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661368.4486 - mae: 206.1863 - mse: 1661369.6250 - val_loss: 228028.4326 - val_mae: 191.3058 - val_mse: 228028.4062\n",
      "Epoch 292/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661363.3436 - mae: 206.1274 - mse: 1661363.0000 - val_loss: 228025.2136 - val_mae: 191.2951 - val_mse: 228025.3281\n",
      "Epoch 293/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661357.0045 - mae: 206.2593 - mse: 1661356.0000 - val_loss: 228023.8001 - val_mae: 191.3142 - val_mse: 228023.7656\n",
      "Epoch 294/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661352.6305 - mae: 206.2403 - mse: 1661352.6250 - val_loss: 228022.0747 - val_mae: 191.3303 - val_mse: 228022.0625\n",
      "Epoch 295/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661347.4532 - mae: 206.3021 - mse: 1661348.6250 - val_loss: 228020.0070 - val_mae: 191.3304 - val_mse: 228020.0781\n",
      "Epoch 296/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661342.8734 - mae: 206.2728 - mse: 1661344.7500 - val_loss: 228018.7454 - val_mae: 191.3578 - val_mse: 228018.8125\n",
      "Epoch 297/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661338.4873 - mae: 206.2514 - mse: 1661339.7500 - val_loss: 228016.0457 - val_mae: 191.3504 - val_mse: 228016.1406\n",
      "Epoch 298/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661332.9289 - mae: 206.2584 - mse: 1661334.6250 - val_loss: 228014.3238 - val_mae: 191.3632 - val_mse: 228014.2500\n",
      "Epoch 299/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661328.0212 - mae: 206.3380 - mse: 1661326.3750 - val_loss: 228012.6822 - val_mae: 191.3765 - val_mse: 228012.7188\n",
      "Epoch 300/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661323.6334 - mae: 206.2721 - mse: 1661323.3750 - val_loss: 228010.5083 - val_mae: 191.3813 - val_mse: 228010.4531\n",
      "Epoch 301/1000\n",
      "45280/46800 [============================>.] - ETA: 0s - loss: 1709391.3673 - mae: 206.4888 - mse: 1709392.1250\n",
      "Epoch: 300, loss:1661318.3840,  mae:206.3111,  mse:1661319.7500,  val_loss:228008.9018,  val_mae:191.3921,  val_mse:228008.9531,  \n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661318.3840 - mae: 206.3111 - mse: 1661319.7500 - val_loss: 228008.9018 - val_mae: 191.3921 - val_mse: 228008.9531\n",
      "Epoch 302/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661314.1429 - mae: 206.3388 - mse: 1661314.8750 - val_loss: 228006.8541 - val_mae: 191.3936 - val_mse: 228006.7969\n",
      "Epoch 303/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661308.8931 - mae: 206.3993 - mse: 1661308.2500 - val_loss: 228005.6862 - val_mae: 191.4190 - val_mse: 228005.6719\n",
      "Epoch 304/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661304.7702 - mae: 206.4078 - mse: 1661303.5000 - val_loss: 228003.7595 - val_mae: 191.4178 - val_mse: 228003.7500\n",
      "Epoch 305/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661299.9888 - mae: 206.4219 - mse: 1661299.2500 - val_loss: 228002.3236 - val_mae: 191.4367 - val_mse: 228002.3594\n",
      "Epoch 306/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661295.6473 - mae: 206.3111 - mse: 1661295.6250 - val_loss: 227999.2992 - val_mae: 191.4234 - val_mse: 227999.3750\n",
      "Epoch 307/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661291.3324 - mae: 206.2201 - mse: 1661291.0000 - val_loss: 227995.7355 - val_mae: 191.3774 - val_mse: 227995.7500\n",
      "Epoch 308/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661286.0601 - mae: 206.2374 - mse: 1661285.3750 - val_loss: 227994.6195 - val_mae: 191.4022 - val_mse: 227994.5781\n",
      "Epoch 309/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661281.2025 - mae: 206.2848 - mse: 1661280.3750 - val_loss: 227993.3395 - val_mae: 191.4190 - val_mse: 227993.3594\n",
      "Epoch 310/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661277.3494 - mae: 206.3389 - mse: 1661276.6250 - val_loss: 227991.5178 - val_mae: 191.4183 - val_mse: 227991.5156\n",
      "Epoch 311/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661273.0346 - mae: 206.2874 - mse: 1661273.5000 - val_loss: 227989.1883 - val_mae: 191.4125 - val_mse: 227989.2031\n",
      "Epoch 312/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661267.8402 - mae: 206.2599 - mse: 1661267.6250 - val_loss: 227988.0961 - val_mae: 191.4379 - val_mse: 227988.0938\n",
      "Epoch 313/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1661263.3541 - mae: 206.2534 - mse: 1661262.5000 - val_loss: 227986.7975 - val_mae: 191.4596 - val_mse: 227986.7500\n",
      "Epoch 314/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661258.7688 - mae: 206.4537 - mse: 1661261.2500 - val_loss: 227985.0317 - val_mae: 191.4551 - val_mse: 227985.0938\n",
      "Epoch 315/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661255.1113 - mae: 206.3739 - mse: 1661255.8750 - val_loss: 227983.4650 - val_mae: 191.4698 - val_mse: 227983.4219\n",
      "Epoch 316/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661250.0287 - mae: 206.3358 - mse: 1661249.8750 - val_loss: 227982.2942 - val_mae: 191.4930 - val_mse: 227982.2812\n",
      "Epoch 317/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661246.2295 - mae: 206.3269 - mse: 1661246.2500 - val_loss: 227978.8918 - val_mae: 191.4519 - val_mse: 227978.9375\n",
      "Epoch 318/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661240.9012 - mae: 206.4298 - mse: 1661241.1250 - val_loss: 227978.3863 - val_mae: 191.4850 - val_mse: 227978.4531\n",
      "Epoch 319/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661237.2723 - mae: 206.3972 - mse: 1661236.2500 - val_loss: 227976.5246 - val_mae: 191.4788 - val_mse: 227976.4688\n",
      "Epoch 320/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661233.2207 - mae: 206.4091 - mse: 1661235.7500 - val_loss: 227975.4806 - val_mae: 191.5039 - val_mse: 227975.4688\n",
      "Epoch 321/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661228.7660 - mae: 206.4265 - mse: 1661230.8750 - val_loss: 227973.8482 - val_mae: 191.5084 - val_mse: 227973.7969\n",
      "Epoch 322/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661224.1613 - mae: 206.4770 - mse: 1661223.6250 - val_loss: 227972.6197 - val_mae: 191.5251 - val_mse: 227972.5938\n",
      "Epoch 323/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661219.6698 - mae: 206.5074 - mse: 1661220.0000 - val_loss: 227971.6421 - val_mae: 191.5502 - val_mse: 227971.7344\n",
      "Epoch 324/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661215.6167 - mae: 206.4759 - mse: 1661214.5000 - val_loss: 227969.9806 - val_mae: 191.5535 - val_mse: 227969.9062\n",
      "Epoch 325/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661211.8112 - mae: 206.4276 - mse: 1661211.7500 - val_loss: 227968.1528 - val_mae: 191.5569 - val_mse: 227968.0469\n",
      "Epoch 326/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661206.8198 - mae: 206.3848 - mse: 1661206.8750 - val_loss: 227966.9346 - val_mae: 191.5770 - val_mse: 227966.9531\n",
      "Epoch 327/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661202.2461 - mae: 206.5180 - mse: 1661202.1250 - val_loss: 227965.9732 - val_mae: 191.5939 - val_mse: 227965.9844\n",
      "Epoch 328/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661198.2733 - mae: 206.5729 - mse: 1661198.6250 - val_loss: 227963.8370 - val_mae: 191.5756 - val_mse: 227963.7812\n",
      "Epoch 329/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661195.3288 - mae: 206.4597 - mse: 1661195.5000 - val_loss: 227960.5076 - val_mae: 191.5356 - val_mse: 227960.4531\n",
      "Epoch 330/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661190.3631 - mae: 206.5435 - mse: 1661190.2500 - val_loss: 227959.0913 - val_mae: 191.5371 - val_mse: 227958.9375\n",
      "Epoch 331/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661186.9657 - mae: 206.4855 - mse: 1661187.2500 - val_loss: 227957.6679 - val_mae: 191.5410 - val_mse: 227957.7969\n",
      "Epoch 332/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661182.6155 - mae: 206.5695 - mse: 1661183.3750 - val_loss: 227956.8883 - val_mae: 191.5675 - val_mse: 227956.9062\n",
      "Epoch 333/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661179.8287 - mae: 206.4134 - mse: 1661180.3750 - val_loss: 227954.1430 - val_mae: 191.5390 - val_mse: 227954.0781\n",
      "Epoch 334/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661174.8691 - mae: 206.4654 - mse: 1661175.1250 - val_loss: 227952.8108 - val_mae: 191.5444 - val_mse: 227952.7969\n",
      "Epoch 335/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661170.9794 - mae: 206.4948 - mse: 1661170.2500 - val_loss: 227951.8283 - val_mae: 191.5612 - val_mse: 227951.6719\n",
      "Epoch 336/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661167.1394 - mae: 206.5387 - mse: 1661167.1250 - val_loss: 227950.1291 - val_mae: 191.5555 - val_mse: 227950.1875\n",
      "Epoch 337/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661163.5992 - mae: 206.4539 - mse: 1661164.5000 - val_loss: 227948.1930 - val_mae: 191.5512 - val_mse: 227948.2344\n",
      "Epoch 338/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661158.7202 - mae: 206.5601 - mse: 1661158.8750 - val_loss: 227946.7185 - val_mae: 191.5486 - val_mse: 227946.7344\n",
      "Epoch 339/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661155.9378 - mae: 206.4699 - mse: 1661156.6250 - val_loss: 227944.4181 - val_mae: 191.5297 - val_mse: 227944.3750\n",
      "Epoch 340/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661151.4059 - mae: 206.5009 - mse: 1661150.8750 - val_loss: 227943.5689 - val_mae: 191.5490 - val_mse: 227943.5312\n",
      "Epoch 341/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661148.0788 - mae: 206.3572 - mse: 1661148.3750 - val_loss: 227941.5335 - val_mae: 191.5368 - val_mse: 227941.4531\n",
      "Epoch 342/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661143.8550 - mae: 206.4506 - mse: 1661142.2500 - val_loss: 227940.9188 - val_mae: 191.5631 - val_mse: 227940.7812\n",
      "Epoch 343/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661139.7546 - mae: 206.5267 - mse: 1661138.0000 - val_loss: 227939.3595 - val_mae: 191.5572 - val_mse: 227939.4062\n",
      "Epoch 344/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661136.6851 - mae: 206.4638 - mse: 1661137.5000 - val_loss: 227938.6802 - val_mae: 191.5829 - val_mse: 227938.7500\n",
      "Epoch 345/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661132.4577 - mae: 206.5576 - mse: 1661132.6250 - val_loss: 227937.7976 - val_mae: 191.6026 - val_mse: 227937.8125\n",
      "Epoch 346/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661128.5982 - mae: 206.5356 - mse: 1661130.0000 - val_loss: 227936.7003 - val_mae: 191.6147 - val_mse: 227936.7344\n",
      "Epoch 347/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1661125.0121 - mae: 206.5048 - mse: 1661125.1250 - val_loss: 227935.6168 - val_mae: 191.6339 - val_mse: 227935.5781\n",
      "Epoch 348/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661120.2411 - mae: 206.6118 - mse: 1661120.2500 - val_loss: 227934.7974 - val_mae: 191.6466 - val_mse: 227934.8281\n",
      "Epoch 349/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661116.8342 - mae: 206.6261 - mse: 1661115.8750 - val_loss: 227933.2890 - val_mae: 191.6387 - val_mse: 227933.2969\n",
      "Epoch 350/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661113.4092 - mae: 206.6234 - mse: 1661113.7500 - val_loss: 227932.7943 - val_mae: 191.6696 - val_mse: 227932.7031\n",
      "Epoch 351/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661109.8355 - mae: 206.6501 - mse: 1661108.8750 - val_loss: 227931.5121 - val_mae: 191.6727 - val_mse: 227931.4844\n",
      "Epoch 352/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661106.3050 - mae: 206.5030 - mse: 1661107.6250 - val_loss: 227929.9799 - val_mae: 191.6737 - val_mse: 227929.9531\n",
      "Epoch 353/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661102.1898 - mae: 206.5886 - mse: 1661102.3750 - val_loss: 227928.9404 - val_mae: 191.6824 - val_mse: 227928.9219\n",
      "Epoch 354/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661098.4083 - mae: 206.6815 - mse: 1661099.2500 - val_loss: 227927.9654 - val_mae: 191.6908 - val_mse: 227927.8906\n",
      "Epoch 355/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661094.9840 - mae: 206.6491 - mse: 1661096.7500 - val_loss: 227926.9904 - val_mae: 191.7022 - val_mse: 227926.8438\n",
      "Epoch 356/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661091.5258 - mae: 206.6549 - mse: 1661091.8750 - val_loss: 227925.6663 - val_mae: 191.7056 - val_mse: 227925.6250\n",
      "Epoch 357/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661087.7859 - mae: 206.6246 - mse: 1661087.8750 - val_loss: 227924.2655 - val_mae: 191.7108 - val_mse: 227924.2344\n",
      "Epoch 358/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661083.7615 - mae: 206.6759 - mse: 1661082.0000 - val_loss: 227923.4825 - val_mae: 191.7215 - val_mse: 227923.4688\n",
      "Epoch 359/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661080.5833 - mae: 206.6451 - mse: 1661080.7500 - val_loss: 227923.0473 - val_mae: 191.7502 - val_mse: 227923.1250\n",
      "Epoch 360/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661076.9714 - mae: 206.6644 - mse: 1661077.8750 - val_loss: 227921.8142 - val_mae: 191.7520 - val_mse: 227921.7500\n",
      "Epoch 361/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661073.6937 - mae: 206.6600 - mse: 1661074.8750 - val_loss: 227920.1533 - val_mae: 191.7407 - val_mse: 227920.0781\n",
      "Epoch 362/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661070.0010 - mae: 206.6372 - mse: 1661070.2500 - val_loss: 227919.5271 - val_mae: 191.7604 - val_mse: 227919.5625\n",
      "Epoch 363/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661066.2723 - mae: 206.7291 - mse: 1661066.6250 - val_loss: 227918.8822 - val_mae: 191.7767 - val_mse: 227918.8594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 364/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661063.1584 - mae: 206.6595 - mse: 1661062.6250 - val_loss: 227917.5746 - val_mae: 191.7788 - val_mse: 227917.5938\n",
      "Epoch 365/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661059.5753 - mae: 206.6336 - mse: 1661060.0000 - val_loss: 227916.4387 - val_mae: 191.7842 - val_mse: 227916.3906\n",
      "Epoch 366/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661055.8800 - mae: 206.7288 - mse: 1661056.2500 - val_loss: 227915.2217 - val_mae: 191.7853 - val_mse: 227915.2656\n",
      "Epoch 367/1000\n",
      "46800/46800 [==============================] - 1s 29us/sample - loss: 1661051.9752 - mae: 206.6936 - mse: 1661051.8750 - val_loss: 227914.7625 - val_mae: 191.8062 - val_mse: 227914.6719\n",
      "Epoch 368/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661048.8186 - mae: 206.7768 - mse: 1661047.2500 - val_loss: 227913.7538 - val_mae: 191.8094 - val_mse: 227913.7656\n",
      "Epoch 369/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1661045.8745 - mae: 206.7415 - mse: 1661044.8750 - val_loss: 227912.7982 - val_mae: 191.8188 - val_mse: 227912.9219\n",
      "Epoch 370/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661042.8418 - mae: 206.6860 - mse: 1661043.7500 - val_loss: 227911.5014 - val_mae: 191.8160 - val_mse: 227911.4688\n",
      "Epoch 371/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661039.1610 - mae: 206.7628 - mse: 1661040.3750 - val_loss: 227910.6590 - val_mae: 191.8291 - val_mse: 227910.6562\n",
      "Epoch 372/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1661035.9447 - mae: 206.7929 - mse: 1661036.1250 - val_loss: 227909.0883 - val_mae: 191.8152 - val_mse: 227909.0156\n",
      "Epoch 373/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661032.9219 - mae: 206.7265 - mse: 1661033.5000 - val_loss: 227908.1512 - val_mae: 191.8275 - val_mse: 227908.1719\n",
      "Epoch 374/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661029.2334 - mae: 206.7434 - mse: 1661028.5000 - val_loss: 227906.4186 - val_mae: 191.8083 - val_mse: 227906.4688\n",
      "Epoch 375/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661025.9942 - mae: 206.7386 - mse: 1661025.5000 - val_loss: 227905.4473 - val_mae: 191.8144 - val_mse: 227905.5469\n",
      "Epoch 376/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661022.4116 - mae: 206.7715 - mse: 1661020.7500 - val_loss: 227903.9994 - val_mae: 191.8008 - val_mse: 227903.9844\n",
      "Epoch 377/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661019.4857 - mae: 206.7986 - mse: 1661020.2500 - val_loss: 227903.1264 - val_mae: 191.8079 - val_mse: 227903.1875\n",
      "Epoch 378/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661016.4764 - mae: 206.7213 - mse: 1661017.2500 - val_loss: 227902.0288 - val_mae: 191.8109 - val_mse: 227902.0312\n",
      "Epoch 379/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1661013.1358 - mae: 206.7461 - mse: 1661012.0000 - val_loss: 227900.9298 - val_mae: 191.8138 - val_mse: 227900.9375\n",
      "Epoch 380/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661009.5227 - mae: 206.7301 - mse: 1661010.2500 - val_loss: 227899.9091 - val_mae: 191.8153 - val_mse: 227899.8906\n",
      "Epoch 381/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661006.7001 - mae: 206.7446 - mse: 1661007.0000 - val_loss: 227899.6488 - val_mae: 191.8452 - val_mse: 227899.6406\n",
      "Epoch 382/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661002.6820 - mae: 206.9054 - mse: 1661001.7500 - val_loss: 227897.7520 - val_mae: 191.8145 - val_mse: 227897.7031\n",
      "Epoch 383/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1661000.0236 - mae: 206.8096 - mse: 1661000.8750 - val_loss: 227897.8664 - val_mae: 191.8521 - val_mse: 227897.8125\n",
      "Epoch 384/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660997.5163 - mae: 206.6818 - mse: 1660996.2500 - val_loss: 227896.3717 - val_mae: 191.8453 - val_mse: 227896.4062\n",
      "Epoch 385/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660993.2383 - mae: 206.8045 - mse: 1660994.1250 - val_loss: 227896.1925 - val_mae: 191.8701 - val_mse: 227896.2344\n",
      "Epoch 386/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1660990.7945 - mae: 206.7563 - mse: 1660990.8750 - val_loss: 227893.9078 - val_mae: 191.8341 - val_mse: 227893.9531\n",
      "Epoch 387/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660986.9460 - mae: 206.8428 - mse: 1660989.6250 - val_loss: 227894.2707 - val_mae: 191.8737 - val_mse: 227894.2344\n",
      "Epoch 388/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660984.6318 - mae: 206.7218 - mse: 1660985.7500 - val_loss: 227893.3854 - val_mae: 191.8810 - val_mse: 227893.3594\n",
      "Epoch 389/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1660981.0559 - mae: 206.8000 - mse: 1660982.2500 - val_loss: 227892.4335 - val_mae: 191.8803 - val_mse: 227892.4531\n",
      "Epoch 390/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1660978.1999 - mae: 206.8793 - mse: 1660977.3750 - val_loss: 227891.6280 - val_mae: 191.8869 - val_mse: 227891.6562\n",
      "Epoch 391/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660975.1348 - mae: 206.8291 - mse: 1660974.0000 - val_loss: 227891.4520 - val_mae: 191.9154 - val_mse: 227891.5000\n",
      "Epoch 392/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1660971.9950 - mae: 206.8095 - mse: 1660972.2500 - val_loss: 227890.7649 - val_mae: 191.9259 - val_mse: 227890.7188\n",
      "Epoch 393/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660968.7595 - mae: 206.8637 - mse: 1660968.6250 - val_loss: 227889.8326 - val_mae: 191.9264 - val_mse: 227889.8438\n",
      "Epoch 394/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1660965.7604 - mae: 206.9224 - mse: 1660964.7500 - val_loss: 227889.6298 - val_mae: 191.9486 - val_mse: 227889.6094\n",
      "Epoch 395/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1660962.7140 - mae: 206.9198 - mse: 1660961.3750 - val_loss: 227888.8039 - val_mae: 191.9532 - val_mse: 227888.7656\n",
      "Epoch 396/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1660959.9885 - mae: 206.8268 - mse: 1660961.1250 - val_loss: 227888.4401 - val_mae: 191.9720 - val_mse: 227888.4844\n",
      "Epoch 397/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660956.4917 - mae: 206.9294 - mse: 1660955.6250 - val_loss: 227887.4613 - val_mae: 191.9697 - val_mse: 227887.5312\n",
      "Epoch 398/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660953.6849 - mae: 206.8662 - mse: 1660954.7500 - val_loss: 227886.9160 - val_mae: 191.9869 - val_mse: 227886.9375\n",
      "Epoch 399/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660950.3561 - mae: 206.9719 - mse: 1660949.0000 - val_loss: 227886.3940 - val_mae: 191.9984 - val_mse: 227886.5000\n",
      "Epoch 400/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1660947.5388 - mae: 206.9657 - mse: 1660946.7500 - val_loss: 227884.8726 - val_mae: 191.9801 - val_mse: 227884.8125\n",
      "Epoch 401/1000\n",
      "45920/46800 [============================>.] - ETA: 0s - loss: 1688689.7117 - mae: 207.5611 - mse: 1688690.3750\n",
      "Epoch: 400, loss:1660944.6603,  mae:206.9525,  mse:1660945.3750,  val_loss:227884.3201,  val_mae:191.9906,  val_mse:227884.2812,  \n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660944.6603 - mae: 206.9525 - mse: 1660945.3750 - val_loss: 227884.3201 - val_mae: 191.9906 - val_mse: 227884.2812\n",
      "Epoch 402/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660942.0123 - mae: 206.9373 - mse: 1660943.0000 - val_loss: 227883.8217 - val_mae: 192.0002 - val_mse: 227883.7812\n",
      "Epoch 403/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1660938.7601 - mae: 206.9684 - mse: 1660939.3750 - val_loss: 227883.0429 - val_mae: 192.0061 - val_mse: 227883.0156\n",
      "Epoch 404/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660936.3116 - mae: 206.9291 - mse: 1660935.5000 - val_loss: 227882.4882 - val_mae: 192.0173 - val_mse: 227882.4062\n",
      "Epoch 405/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1660933.6781 - mae: 206.9072 - mse: 1660933.7500 - val_loss: 227881.9551 - val_mae: 192.0303 - val_mse: 227881.9219\n",
      "Epoch 406/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1660929.9190 - mae: 206.9684 - mse: 1660929.3750 - val_loss: 227881.6665 - val_mae: 192.0469 - val_mse: 227881.6094\n",
      "Epoch 407/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660927.2090 - mae: 207.0092 - mse: 1660926.3750 - val_loss: 227881.1268 - val_mae: 192.0556 - val_mse: 227881.1406\n",
      "Epoch 408/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660924.5728 - mae: 206.9915 - mse: 1660923.1250 - val_loss: 227880.7048 - val_mae: 192.0717 - val_mse: 227880.6562\n",
      "Epoch 409/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660921.3419 - mae: 207.0757 - mse: 1660923.2500 - val_loss: 227880.6586 - val_mae: 192.0956 - val_mse: 227880.7031\n",
      "Epoch 410/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1660919.2175 - mae: 206.9809 - mse: 1660919.6250 - val_loss: 227879.0902 - val_mae: 192.0783 - val_mse: 227879.0938\n",
      "Epoch 411/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1660916.1571 - mae: 206.9859 - mse: 1660915.0000 - val_loss: 227878.8446 - val_mae: 192.0957 - val_mse: 227878.8594\n",
      "Epoch 412/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660913.1408 - mae: 207.0695 - mse: 1660911.8750 - val_loss: 227877.7889 - val_mae: 192.0875 - val_mse: 227877.8750\n",
      "Epoch 413/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660910.4773 - mae: 206.9341 - mse: 1660910.3750 - val_loss: 227877.2792 - val_mae: 192.1030 - val_mse: 227877.2812\n",
      "Epoch 414/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660907.1241 - mae: 207.0803 - mse: 1660906.0000 - val_loss: 227876.3049 - val_mae: 192.0953 - val_mse: 227876.2344\n",
      "Epoch 415/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660904.6757 - mae: 207.0638 - mse: 1660905.6250 - val_loss: 227876.0307 - val_mae: 192.1144 - val_mse: 227876.0000\n",
      "Epoch 416/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660901.5880 - mae: 207.1219 - mse: 1660901.6250 - val_loss: 227876.3223 - val_mae: 192.1465 - val_mse: 227876.2969\n",
      "Epoch 417/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660898.9111 - mae: 207.0670 - mse: 1660898.7500 - val_loss: 227875.4299 - val_mae: 192.1437 - val_mse: 227875.5156\n",
      "Epoch 418/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660896.4465 - mae: 207.0977 - mse: 1660896.0000 - val_loss: 227874.3872 - val_mae: 192.1367 - val_mse: 227874.3594\n",
      "Epoch 419/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1660893.1975 - mae: 207.1376 - mse: 1660892.5000 - val_loss: 227874.4466 - val_mae: 192.1626 - val_mse: 227874.4688\n",
      "Epoch 420/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660891.0528 - mae: 207.0623 - mse: 1660891.7500 - val_loss: 227873.1020 - val_mae: 192.1459 - val_mse: 227873.1562\n",
      "Epoch 421/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660887.9705 - mae: 207.1167 - mse: 1660888.6250 - val_loss: 227872.3441 - val_mae: 192.1452 - val_mse: 227872.2500\n",
      "Epoch 422/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660885.5564 - mae: 207.0982 - mse: 1660886.0000 - val_loss: 227872.0269 - val_mae: 192.1577 - val_mse: 227872.0781\n",
      "Epoch 423/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660883.6863 - mae: 207.0006 - mse: 1660882.5000 - val_loss: 227870.6357 - val_mae: 192.1401 - val_mse: 227870.7656\n",
      "Epoch 424/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1660879.9000 - mae: 207.1539 - mse: 1660879.7500 - val_loss: 227869.3427 - val_mae: 192.1228 - val_mse: 227869.4062\n",
      "Epoch 425/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660878.6742 - mae: 206.9496 - mse: 1660878.5000 - val_loss: 227868.1735 - val_mae: 192.1185 - val_mse: 227868.1875\n",
      "Epoch 426/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1660874.8232 - mae: 207.0921 - mse: 1660873.0000 - val_loss: 227868.5775 - val_mae: 192.1531 - val_mse: 227868.5781\n",
      "Epoch 427/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1660872.9241 - mae: 207.0086 - mse: 1660872.5000 - val_loss: 227867.4068 - val_mae: 192.1430 - val_mse: 227867.4375\n",
      "Epoch 428/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660869.6030 - mae: 207.1109 - mse: 1660869.1250 - val_loss: 227867.3264 - val_mae: 192.1605 - val_mse: 227867.4844\n",
      "Epoch 429/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660867.0091 - mae: 207.1341 - mse: 1660868.6250 - val_loss: 227866.7868 - val_mae: 192.1662 - val_mse: 227866.7812\n",
      "Epoch 430/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660864.6163 - mae: 207.1086 - mse: 1660862.7500 - val_loss: 227866.3898 - val_mae: 192.1764 - val_mse: 227866.4375\n",
      "Epoch 431/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660861.8964 - mae: 207.1351 - mse: 1660862.0000 - val_loss: 227866.3976 - val_mae: 192.2003 - val_mse: 227866.4375\n",
      "Epoch 432/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660858.9198 - mae: 207.1776 - mse: 1660856.7500 - val_loss: 227865.9208 - val_mae: 192.2077 - val_mse: 227865.8125\n",
      "Epoch 433/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660857.6879 - mae: 207.0382 - mse: 1660859.0000 - val_loss: 227864.1978 - val_mae: 192.1814 - val_mse: 227864.1406\n",
      "Epoch 434/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1660854.3533 - mae: 207.0600 - mse: 1660854.2500 - val_loss: 227863.8542 - val_mae: 192.1902 - val_mse: 227863.7812\n",
      "Epoch 435/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1660851.0534 - mae: 207.2444 - mse: 1660852.3750 - val_loss: 227863.3068 - val_mae: 192.1920 - val_mse: 227863.3281\n",
      "Epoch 436/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660849.5594 - mae: 207.1303 - mse: 1660851.0000 - val_loss: 227861.4852 - val_mae: 192.1625 - val_mse: 227861.5469\n",
      "Epoch 437/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660846.8983 - mae: 207.2130 - mse: 1660847.6250 - val_loss: 227862.0766 - val_mae: 192.1978 - val_mse: 227862.0625\n",
      "Epoch 438/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660844.5105 - mae: 207.1598 - mse: 1660845.0000 - val_loss: 227861.4019 - val_mae: 192.1972 - val_mse: 227861.4219\n",
      "Epoch 439/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1660842.1611 - mae: 207.1134 - mse: 1660843.5000 - val_loss: 227860.8343 - val_mae: 192.2063 - val_mse: 227860.9219\n",
      "Epoch 440/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660839.9937 - mae: 207.0717 - mse: 1660840.1250 - val_loss: 227859.9590 - val_mae: 192.2017 - val_mse: 227860.0469\n",
      "Epoch 441/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660837.2572 - mae: 207.0953 - mse: 1660836.7500 - val_loss: 227859.7969 - val_mae: 192.2173 - val_mse: 227859.7812\n",
      "Epoch 442/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660834.6646 - mae: 207.1409 - mse: 1660836.1250 - val_loss: 227859.5255 - val_mae: 192.2296 - val_mse: 227859.6250\n",
      "Epoch 443/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660832.4755 - mae: 207.0435 - mse: 1660832.7500 - val_loss: 227858.1308 - val_mae: 192.2097 - val_mse: 227858.2656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 444/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660829.1701 - mae: 207.2306 - mse: 1660829.7500 - val_loss: 227858.7667 - val_mae: 192.2453 - val_mse: 227858.7969\n",
      "Epoch 445/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660826.9185 - mae: 207.3141 - mse: 1660826.0000 - val_loss: 227857.8156 - val_mae: 192.2342 - val_mse: 227857.8906\n",
      "Epoch 446/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660826.0925 - mae: 207.0974 - mse: 1660826.8750 - val_loss: 227855.5155 - val_mae: 192.1908 - val_mse: 227855.4844\n",
      "Epoch 447/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660822.7948 - mae: 207.1958 - mse: 1660822.1250 - val_loss: 227854.9681 - val_mae: 192.1907 - val_mse: 227854.9062\n",
      "Epoch 448/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660820.8415 - mae: 207.1370 - mse: 1660822.1250 - val_loss: 227855.3081 - val_mae: 192.2206 - val_mse: 227855.2031\n",
      "Epoch 449/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1660818.2670 - mae: 207.2037 - mse: 1660819.8750 - val_loss: 227855.2623 - val_mae: 192.2396 - val_mse: 227855.2344\n",
      "Epoch 450/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660816.2572 - mae: 207.0299 - mse: 1660816.1250 - val_loss: 227854.4576 - val_mae: 192.2380 - val_mse: 227854.4375\n",
      "Epoch 451/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660813.3734 - mae: 207.1356 - mse: 1660814.6250 - val_loss: 227854.5598 - val_mae: 192.2579 - val_mse: 227854.7500\n",
      "Epoch 452/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1660810.7932 - mae: 207.1864 - mse: 1660811.1250 - val_loss: 227854.7257 - val_mae: 192.2815 - val_mse: 227854.5781\n",
      "Epoch 453/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1660808.0337 - mae: 207.2372 - mse: 1660807.0000 - val_loss: 227854.6920 - val_mae: 192.2969 - val_mse: 227854.7656\n",
      "Epoch 454/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1660805.7362 - mae: 207.2998 - mse: 1660805.6250 - val_loss: 227853.7991 - val_mae: 192.2863 - val_mse: 227853.8125\n",
      "Epoch 455/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660805.1261 - mae: 207.1542 - mse: 1660805.1250 - val_loss: 227852.4783 - val_mae: 192.2687 - val_mse: 227852.4531\n",
      "Epoch 456/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660801.7759 - mae: 207.2002 - mse: 1660804.6250 - val_loss: 227852.3857 - val_mae: 192.2820 - val_mse: 227852.4219\n",
      "Epoch 457/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1660800.2835 - mae: 207.1609 - mse: 1660800.7500 - val_loss: 227851.3882 - val_mae: 192.2707 - val_mse: 227851.4062\n",
      "Epoch 458/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1660797.0898 - mae: 207.1778 - mse: 1660796.5000 - val_loss: 227851.3697 - val_mae: 192.2873 - val_mse: 227851.4062\n",
      "Epoch 459/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660794.7952 - mae: 207.2160 - mse: 1660795.1250 - val_loss: 227851.7739 - val_mae: 192.3145 - val_mse: 227851.9688\n",
      "Epoch 460/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660792.4062 - mae: 207.2686 - mse: 1660793.6250 - val_loss: 227851.3673 - val_mae: 192.3165 - val_mse: 227851.3281\n",
      "Epoch 461/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660789.8240 - mae: 207.3363 - mse: 1660790.2500 - val_loss: 227851.4731 - val_mae: 192.3377 - val_mse: 227851.4219\n",
      "Epoch 462/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660787.5718 - mae: 207.3143 - mse: 1660787.7500 - val_loss: 227851.4700 - val_mae: 192.3516 - val_mse: 227851.5469\n",
      "Epoch 463/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660785.9174 - mae: 207.2434 - mse: 1660786.0000 - val_loss: 227850.9002 - val_mae: 192.3526 - val_mse: 227850.9375\n",
      "Epoch 464/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1660783.8730 - mae: 207.2478 - mse: 1660784.8750 - val_loss: 227849.5517 - val_mae: 192.3295 - val_mse: 227849.5156\n",
      "Epoch 465/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1660780.6164 - mae: 207.3423 - mse: 1660782.3750 - val_loss: 227849.5786 - val_mae: 192.3456 - val_mse: 227849.5469\n",
      "Epoch 466/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660778.8122 - mae: 207.3520 - mse: 1660781.1250 - val_loss: 227849.0358 - val_mae: 192.3444 - val_mse: 227849.0000\n",
      "Epoch 467/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1660777.1315 - mae: 207.2991 - mse: 1660776.6250 - val_loss: 227848.7430 - val_mae: 192.3542 - val_mse: 227848.6719\n",
      "Epoch 468/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660774.4615 - mae: 207.3410 - mse: 1660775.0000 - val_loss: 227848.5450 - val_mae: 192.3623 - val_mse: 227848.4531\n",
      "Epoch 469/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660772.5349 - mae: 207.3030 - mse: 1660773.3750 - val_loss: 227848.1034 - val_mae: 192.3642 - val_mse: 227848.0781\n",
      "Epoch 470/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660771.2397 - mae: 207.1632 - mse: 1660771.6250 - val_loss: 227845.6967 - val_mae: 192.3118 - val_mse: 227845.6406\n",
      "Epoch 471/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1660768.6213 - mae: 207.1842 - mse: 1660768.1250 - val_loss: 227847.1618 - val_mae: 192.3698 - val_mse: 227847.1094\n",
      "Epoch 472/1000\n",
      "46800/46800 [==============================] - 1s 30us/sample - loss: 1660766.0725 - mae: 207.2897 - mse: 1660765.3750 - val_loss: 227846.9048 - val_mae: 192.3762 - val_mse: 227846.8125\n",
      "Epoch 473/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660763.8853 - mae: 207.2841 - mse: 1660763.2500 - val_loss: 227846.5673 - val_mae: 192.3796 - val_mse: 227846.6719\n",
      "Epoch 474/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660761.6222 - mae: 207.3336 - mse: 1660762.0000 - val_loss: 227846.7184 - val_mae: 192.4010 - val_mse: 227846.6250\n",
      "Epoch 475/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1660759.0313 - mae: 207.4182 - mse: 1660758.7500 - val_loss: 227846.7410 - val_mae: 192.4160 - val_mse: 227846.7812\n",
      "Epoch 476/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660757.0314 - mae: 207.4356 - mse: 1660756.2500 - val_loss: 227846.6243 - val_mae: 192.4269 - val_mse: 227846.6250\n",
      "Epoch 477/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660755.1254 - mae: 207.4620 - mse: 1660753.6250 - val_loss: 227846.7286 - val_mae: 192.4458 - val_mse: 227846.7188\n",
      "Epoch 478/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1660753.1547 - mae: 207.3928 - mse: 1660751.5000 - val_loss: 227846.5779 - val_mae: 192.4589 - val_mse: 227846.5625\n",
      "Epoch 479/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660751.6226 - mae: 207.2865 - mse: 1660751.0000 - val_loss: 227843.6838 - val_mae: 192.3914 - val_mse: 227843.7656\n",
      "Epoch 480/1000\n",
      "46800/46800 [==============================] - 1s 32us/sample - loss: 1660748.7168 - mae: 207.3111 - mse: 1660747.3750 - val_loss: 227845.6556 - val_mae: 192.4611 - val_mse: 227845.6875\n",
      "Epoch 481/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1660746.7055 - mae: 207.3458 - mse: 1660747.1250 - val_loss: 227845.5829 - val_mae: 192.4727 - val_mse: 227845.5312\n",
      "Epoch 482/1000\n",
      "46800/46800 [==============================] - 2s 36us/sample - loss: 1660744.3137 - mae: 207.4204 - mse: 1660743.5000 - val_loss: 227845.2473 - val_mae: 192.4752 - val_mse: 227845.1250\n",
      "Epoch 483/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1660742.6157 - mae: 207.4015 - mse: 1660743.6250 - val_loss: 227845.0352 - val_mae: 192.4814 - val_mse: 227845.0312\n",
      "Epoch 484/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660740.9383 - mae: 207.3975 - mse: 1660742.7500 - val_loss: 227844.5110 - val_mae: 192.4789 - val_mse: 227844.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 485/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660738.1426 - mae: 207.4838 - mse: 1660739.5000 - val_loss: 227844.8298 - val_mae: 192.5007 - val_mse: 227844.7969\n",
      "Epoch 486/1000\n",
      "46800/46800 [==============================] - 2s 35us/sample - loss: 1660737.0906 - mae: 207.3590 - mse: 1660737.2500 - val_loss: 227843.4159 - val_mae: 192.4724 - val_mse: 227843.3438\n",
      "Epoch 487/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660734.3308 - mae: 207.4500 - mse: 1660734.7500 - val_loss: 227843.9837 - val_mae: 192.5015 - val_mse: 227843.9844\n",
      "Epoch 488/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1660732.5024 - mae: 207.4203 - mse: 1660730.7500 - val_loss: 227844.4819 - val_mae: 192.5293 - val_mse: 227844.4688\n",
      "Epoch 489/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1660729.8618 - mae: 207.4704 - mse: 1660729.5000 - val_loss: 227844.7479 - val_mae: 192.5482 - val_mse: 227844.7188\n",
      "Epoch 490/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660728.1815 - mae: 207.4299 - mse: 1660728.7500 - val_loss: 227844.0747 - val_mae: 192.5408 - val_mse: 227844.0625\n",
      "Epoch 491/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660726.5611 - mae: 207.4139 - mse: 1660725.8750 - val_loss: 227843.5860 - val_mae: 192.5397 - val_mse: 227843.6094\n",
      "Epoch 492/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1660723.8508 - mae: 207.4859 - mse: 1660720.7500 - val_loss: 227843.6058 - val_mae: 192.5510 - val_mse: 227843.6250\n",
      "Epoch 493/1000\n",
      "46800/46800 [==============================] - 2s 35us/sample - loss: 1660721.5492 - mae: 207.5959 - mse: 1660719.8750 - val_loss: 227842.6253 - val_mae: 192.5343 - val_mse: 227842.6406\n",
      "Epoch 494/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1660720.6204 - mae: 207.4499 - mse: 1660722.0000 - val_loss: 227841.6212 - val_mae: 192.5179 - val_mse: 227841.5469\n",
      "Epoch 495/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1660718.4457 - mae: 207.5142 - mse: 1660719.0000 - val_loss: 227841.5032 - val_mae: 192.5278 - val_mse: 227841.5781\n",
      "Epoch 496/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660717.1374 - mae: 207.3716 - mse: 1660717.1250 - val_loss: 227840.7431 - val_mae: 192.5173 - val_mse: 227840.8125\n",
      "Epoch 497/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1660714.9102 - mae: 207.4175 - mse: 1660714.5000 - val_loss: 227840.3779 - val_mae: 192.5193 - val_mse: 227840.3281\n",
      "Epoch 498/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1660712.8263 - mae: 207.3827 - mse: 1660713.3750 - val_loss: 227840.7106 - val_mae: 192.5419 - val_mse: 227840.6406\n",
      "Epoch 499/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1660710.6534 - mae: 207.4609 - mse: 1660709.8750 - val_loss: 227841.0934 - val_mae: 192.5636 - val_mse: 227841.0781\n",
      "Epoch 500/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1660709.1301 - mae: 207.5045 - mse: 1660710.3750 - val_loss: 227841.1505 - val_mae: 192.5774 - val_mse: 227841.1094\n",
      "Epoch 501/1000\n",
      "46784/46800 [============================>.] - ETA: 0s - loss: 1661224.3337 - mae: 207.3747 - mse: 1661223.8750\n",
      "Epoch: 500, loss:1660707.4696,  mae:207.3804,  mse:1660707.1250,  val_loss:227839.8678,  val_mae:192.5528,  val_mse:227839.8750,  \n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660707.4696 - mae: 207.3804 - mse: 1660707.1250 - val_loss: 227839.8678 - val_mae: 192.5528 - val_mse: 227839.8750\n",
      "Epoch 502/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660705.1295 - mae: 207.5099 - mse: 1660705.5000 - val_loss: 227839.5309 - val_mae: 192.5533 - val_mse: 227839.5469\n",
      "Epoch 503/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660703.5610 - mae: 207.4162 - mse: 1660703.6250 - val_loss: 227839.2076 - val_mae: 192.5539 - val_mse: 227839.1094\n",
      "Epoch 504/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660700.5450 - mae: 207.6151 - mse: 1660698.1250 - val_loss: 227839.1114 - val_mae: 192.5603 - val_mse: 227839.0781\n",
      "Epoch 505/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660699.4525 - mae: 207.5003 - mse: 1660701.8750 - val_loss: 227839.0544 - val_mae: 192.5695 - val_mse: 227838.9688\n",
      "Epoch 506/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660697.3779 - mae: 207.5519 - mse: 1660697.8750 - val_loss: 227838.4618 - val_mae: 192.5624 - val_mse: 227838.5156\n",
      "Epoch 507/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660696.2473 - mae: 207.4972 - mse: 1660696.1250 - val_loss: 227837.9585 - val_mae: 192.5601 - val_mse: 227837.9219\n",
      "Epoch 508/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1660694.0549 - mae: 207.5089 - mse: 1660693.6250 - val_loss: 227836.9542 - val_mae: 192.5419 - val_mse: 227837.0469\n",
      "Epoch 509/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1660691.7552 - mae: 207.4425 - mse: 1660692.0000 - val_loss: 227838.6668 - val_mae: 192.6044 - val_mse: 227838.5938\n",
      "Epoch 510/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660689.6397 - mae: 207.5740 - mse: 1660689.7500 - val_loss: 227838.6926 - val_mae: 192.6156 - val_mse: 227838.8438\n",
      "Epoch 511/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660688.2187 - mae: 207.5339 - mse: 1660690.8750 - val_loss: 227838.4203 - val_mae: 192.6169 - val_mse: 227838.4844\n",
      "Epoch 512/1000\n",
      "46800/46800 [==============================] - 2s 32us/sample - loss: 1660686.7505 - mae: 207.5914 - mse: 1660684.7500 - val_loss: 227837.2673 - val_mae: 192.5948 - val_mse: 227837.2656\n",
      "Epoch 513/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660684.6705 - mae: 207.5046 - mse: 1660684.1250 - val_loss: 227836.7754 - val_mae: 192.5923 - val_mse: 227836.7188\n",
      "Epoch 514/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660683.5129 - mae: 207.4584 - mse: 1660683.6250 - val_loss: 227834.5239 - val_mae: 192.5365 - val_mse: 227834.4844\n",
      "Epoch 515/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660680.7882 - mae: 207.5641 - mse: 1660682.2500 - val_loss: 227836.4370 - val_mae: 192.6030 - val_mse: 227836.4844\n",
      "Epoch 516/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660679.3493 - mae: 207.5621 - mse: 1660679.6250 - val_loss: 227836.2240 - val_mae: 192.6066 - val_mse: 227836.2344\n",
      "Epoch 517/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660677.0672 - mae: 207.5715 - mse: 1660676.5000 - val_loss: 227837.4484 - val_mae: 192.6536 - val_mse: 227837.4219\n",
      "Epoch 518/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660675.1825 - mae: 207.6059 - mse: 1660676.5000 - val_loss: 227837.9266 - val_mae: 192.6765 - val_mse: 227837.9062\n",
      "Epoch 519/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660673.0530 - mae: 207.6843 - mse: 1660673.3750 - val_loss: 227838.4155 - val_mae: 192.7010 - val_mse: 227838.4219\n",
      "Epoch 520/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660671.4486 - mae: 207.6837 - mse: 1660670.5000 - val_loss: 227838.3573 - val_mae: 192.7077 - val_mse: 227838.3750\n",
      "Epoch 521/1000\n",
      "46800/46800 [==============================] - 2s 34us/sample - loss: 1660669.8597 - mae: 207.5959 - mse: 1660671.0000 - val_loss: 227839.2441 - val_mae: 192.7449 - val_mse: 227839.1875\n",
      "Epoch 522/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660668.8377 - mae: 207.5869 - mse: 1660668.7500 - val_loss: 227837.5651 - val_mae: 192.7053 - val_mse: 227837.6406\n",
      "Epoch 523/1000\n",
      "46800/46800 [==============================] - 2s 33us/sample - loss: 1660666.0074 - mae: 207.6968 - mse: 1660666.2500 - val_loss: 227837.6771 - val_mae: 192.7157 - val_mse: 227837.5781\n",
      "Epoch 524/1000\n",
      "46800/46800 [==============================] - 1s 31us/sample - loss: 1660664.2677 - mae: 207.7097 - mse: 1660663.0000 - val_loss: 227837.3315 - val_mae: 192.7124 - val_mse: 227837.2812\n",
      "TRAINING ENDED\n",
      "-------------------------------------------------------------------------------\n",
      "/n\n",
      "Strategy net profitability\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "23801.51463142395\n",
      "-------------------------------------------------------------------------------\n",
      "/n\n",
      "the first few reults of our predictions - to check they arent identical\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "[[117.04174 ]\n",
      " [115.8162  ]\n",
      " [167.71275 ]\n",
      " [151.74535 ]\n",
      " [186.37077 ]\n",
      " [-85.16344 ]\n",
      " [162.00575 ]\n",
      " [159.40578 ]\n",
      " [ 87.48956 ]\n",
      " [ 99.196205]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_whole_model_assesment(build_explicit_linear_mse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 46800 samples, validate on 11700 samples\n",
      "Epoch 1/1000\n",
      "46400/46800 [============================>.] - ETA: 0s - loss: 115.7670 - mae: 115.7671 - mse: 1689534.8750\n",
      "Epoch: 0, loss:115.8468,  mae:115.8468,  mse:1678143.5000,  val_loss:100.5620,  val_mae:100.5620,  val_mse:240338.9531,  \n",
      "46800/46800 [==============================] - 8s 180us/sample - loss: 115.8468 - mae: 115.8468 - mse: 1678143.5000 - val_loss: 100.5620 - val_mae: 100.5620 - val_mse: 240338.9531\n",
      "Epoch 2/1000\n",
      "46800/46800 [==============================] - 9s 189us/sample - loss: 115.8459 - mae: 115.8461 - mse: 1678142.3750 - val_loss: 100.5621 - val_mae: 100.5621 - val_mse: 240338.9219\n",
      "Epoch 3/1000\n",
      "46800/46800 [==============================] - 8s 173us/sample - loss: 115.8459 - mae: 115.8459 - mse: 1678144.1250 - val_loss: 100.5621 - val_mae: 100.5621 - val_mse: 240338.9062\n",
      "Epoch 4/1000\n",
      "46800/46800 [==============================] - 8s 169us/sample - loss: 115.8459 - mae: 115.8460 - mse: 1678144.3750 - val_loss: 100.5620 - val_mae: 100.5620 - val_mse: 240338.9062\n",
      "Epoch 5/1000\n",
      "46800/46800 [==============================] - 8s 162us/sample - loss: 115.8459 - mae: 115.8459 - mse: 1678142.8750 - val_loss: 100.5620 - val_mae: 100.5621 - val_mse: 240338.9219\n",
      "Epoch 6/1000\n",
      "46800/46800 [==============================] - 9s 182us/sample - loss: 115.8459 - mae: 115.8458 - mse: 1678145.3750 - val_loss: 100.5620 - val_mae: 100.5620 - val_mse: 240338.9219\n",
      "Epoch 7/1000\n",
      "46800/46800 [==============================] - 10s 211us/sample - loss: 115.8459 - mae: 115.8459 - mse: 1678144.8750 - val_loss: 100.5620 - val_mae: 100.5620 - val_mse: 240339.0312\n",
      "Epoch 8/1000\n",
      "46800/46800 [==============================] - 8s 172us/sample - loss: 115.8459 - mae: 115.8459 - mse: 1678144.5000 - val_loss: 100.5621 - val_mae: 100.5621 - val_mse: 240338.9844\n",
      "Epoch 9/1000\n",
      "46800/46800 [==============================] - 9s 197us/sample - loss: 115.8459 - mae: 115.8459 - mse: 1678146.3750 - val_loss: 100.5621 - val_mae: 100.5621 - val_mse: 240338.9844\n",
      "Epoch 10/1000\n",
      "46800/46800 [==============================] - 9s 199us/sample - loss: 115.8459 - mae: 115.8459 - mse: 1678144.8750 - val_loss: 100.5621 - val_mae: 100.5620 - val_mse: 240339.0625\n",
      "Epoch 11/1000\n",
      "46800/46800 [==============================] - 9s 200us/sample - loss: 115.8459 - mae: 115.8459 - mse: 1678143.6250 - val_loss: 100.5622 - val_mae: 100.5622 - val_mse: 240338.8750\n",
      "TRAINING ENDED\n",
      "failed to print graph - not that its that good anyway\n",
      "-------------------------------------------------------------------------------\n",
      "/n\n",
      "Strategy net profitability\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "-703598.5589583269\n",
      "-------------------------------------------------------------------------------\n",
      "/n\n",
      "the first few reults of our predictions - to check they arent identical\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "[[0.00031247]\n",
      " [0.00031247]\n",
      " [0.00031247]\n",
      " [0.00031247]\n",
      " [0.00031247]\n",
      " [0.00031247]\n",
      " [0.00031247]\n",
      " [0.00031247]\n",
      " [0.00031247]\n",
      " [0.00031247]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_whole_model_assesment(build_tanh6_relu5_lin_mae())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 46800 samples, validate on 11700 samples\n",
      "Epoch 1/1000\n",
      "46528/46800 [============================>.] - ETA: 0s - loss: 1675945.7530 - mae: 174.6989 - mse: 1675945.0000\n",
      "Epoch: 0, loss:1667257.1847,  mae:174.7238,  mse:1667256.8750,  val_loss:230115.3521,  val_mae:181.0753,  val_mse:230115.3750,  \n",
      "46800/46800 [==============================] - 9s 189us/sample - loss: 1667257.1847 - mae: 174.7238 - mse: 1667256.8750 - val_loss: 230115.3521 - val_mae: 181.0753 - val_mse: 230115.3750\n",
      "Epoch 2/1000\n",
      "46800/46800 [==============================] - 7s 159us/sample - loss: 1663923.4311 - mae: 193.8614 - mse: 1663926.0000 - val_loss: 228945.0211 - val_mae: 190.5552 - val_mse: 228945.0156\n",
      "Epoch 3/1000\n",
      "46800/46800 [==============================] - 10s 219us/sample - loss: 1662310.8366 - mae: 198.8555 - mse: 1662312.3750 - val_loss: 228078.9467 - val_mae: 184.1001 - val_mse: 228078.8906\n",
      "Epoch 4/1000\n",
      "46800/46800 [==============================] - 9s 199us/sample - loss: 1661371.9938 - mae: 199.2893 - mse: 1661371.5000 - val_loss: 227939.3596 - val_mae: 187.3136 - val_mse: 227939.3125\n",
      "Epoch 5/1000\n",
      "46800/46800 [==============================] - 9s 191us/sample - loss: 1660932.3079 - mae: 199.5827 - mse: 1660932.0000 - val_loss: 228011.9774 - val_mae: 191.4500 - val_mse: 228012.0156\n",
      "Epoch 6/1000\n",
      "46800/46800 [==============================] - 10s 219us/sample - loss: 1660469.0722 - mae: 199.2170 - mse: 1660469.7500 - val_loss: 228372.4220 - val_mae: 198.5763 - val_mse: 228372.5312\n",
      "Epoch 7/1000\n",
      "46800/46800 [==============================] - 10s 209us/sample - loss: 1660275.0852 - mae: 202.5079 - mse: 1660276.5000 - val_loss: 227621.1892 - val_mae: 179.7787 - val_mse: 227621.0938\n",
      "Epoch 8/1000\n",
      "46800/46800 [==============================] - 10s 215us/sample - loss: 1660044.5105 - mae: 197.7583 - mse: 1660044.8750 - val_loss: 228391.1178 - val_mae: 199.3248 - val_mse: 228391.0781\n",
      "Epoch 9/1000\n",
      "46800/46800 [==============================] - 10s 207us/sample - loss: 1659885.8711 - mae: 203.0670 - mse: 1659886.1250 - val_loss: 227596.7533 - val_mae: 180.5191 - val_mse: 227596.7812\n",
      "Epoch 10/1000\n",
      "46800/46800 [==============================] - 9s 202us/sample - loss: 1659675.5537 - mae: 201.0830 - mse: 1659673.6250 - val_loss: 227546.4082 - val_mae: 174.9004 - val_mse: 227546.4688\n",
      "Epoch 11/1000\n",
      "46800/46800 [==============================] - 10s 216us/sample - loss: 1659569.9432 - mae: 197.2178 - mse: 1659569.5000 - val_loss: 228627.8376 - val_mae: 202.4861 - val_mse: 228627.9375\n",
      "Epoch 12/1000\n",
      "46800/46800 [==============================] - 10s 205us/sample - loss: 1659434.3671 - mae: 202.4758 - mse: 1659433.8750 - val_loss: 227745.9509 - val_mae: 187.4073 - val_mse: 227745.8438\n",
      "Epoch 13/1000\n",
      "46800/46800 [==============================] - 10s 222us/sample - loss: 1659269.5509 - mae: 200.9834 - mse: 1659271.6250 - val_loss: 227556.7195 - val_mae: 181.3857 - val_mse: 227556.6875\n",
      "Epoch 14/1000\n",
      "46800/46800 [==============================] - 10s 216us/sample - loss: 1659115.5589 - mae: 200.9864 - mse: 1659114.3750 - val_loss: 227528.0601 - val_mae: 180.2851 - val_mse: 227528.1250\n",
      "Epoch 15/1000\n",
      "46800/46800 [==============================] - 11s 230us/sample - loss: 1659047.6342 - mae: 199.7986 - mse: 1659047.5000 - val_loss: 227779.8394 - val_mae: 188.7430 - val_mse: 227779.8906\n",
      "Epoch 16/1000\n",
      "46800/46800 [==============================] - 10s 215us/sample - loss: 1658910.5998 - mae: 201.1394 - mse: 1658912.8750 - val_loss: 227541.3817 - val_mae: 182.4230 - val_mse: 227541.4062\n",
      "Epoch 17/1000\n",
      "46800/46800 [==============================] - 10s 218us/sample - loss: 1658793.2045 - mae: 200.6041 - mse: 1658793.3750 - val_loss: 227705.7829 - val_mae: 188.0035 - val_mse: 227705.8594\n",
      "Epoch 18/1000\n",
      "46800/46800 [==============================] - 11s 225us/sample - loss: 1658644.2062 - mae: 201.3824 - mse: 1658645.2500 - val_loss: 227594.0244 - val_mae: 184.6715 - val_mse: 227593.9531\n",
      "Epoch 19/1000\n",
      "46800/46800 [==============================] - 12s 259us/sample - loss: 1658608.0962 - mae: 200.6330 - mse: 1658606.7500 - val_loss: 227453.3088 - val_mae: 179.7264 - val_mse: 227453.3125\n",
      "Epoch 20/1000\n",
      "46800/46800 [==============================] - 12s 260us/sample - loss: 1658444.7621 - mae: 201.1240 - mse: 1658446.1250 - val_loss: 227500.7650 - val_mae: 181.8754 - val_mse: 227500.8125\n",
      "Epoch 21/1000\n",
      "46800/46800 [==============================] - 10s 211us/sample - loss: 1658271.3441 - mae: 200.1532 - mse: 1658270.3750 - val_loss: 227579.0794 - val_mae: 186.0553 - val_mse: 227579.0781\n",
      "Epoch 22/1000\n",
      "46800/46800 [==============================] - 9s 188us/sample - loss: 1658280.7110 - mae: 201.5899 - mse: 1658282.3750 - val_loss: 227462.4419 - val_mae: 180.6966 - val_mse: 227462.4844\n",
      "Epoch 23/1000\n",
      "46800/46800 [==============================] - 9s 192us/sample - loss: 1658134.6002 - mae: 199.7120 - mse: 1658132.8750 - val_loss: 227958.5241 - val_mae: 192.9338 - val_mse: 227958.4219\n",
      "Epoch 24/1000\n",
      "46800/46800 [==============================] - 9s 187us/sample - loss: 1658063.8604 - mae: 200.6378 - mse: 1658064.1250 - val_loss: 228422.7658 - val_mae: 199.1054 - val_mse: 228422.7188\n",
      "Epoch 25/1000\n",
      "46800/46800 [==============================] - 9s 191us/sample - loss: 1657956.2965 - mae: 202.4804 - mse: 1657955.3750 - val_loss: 227610.9811 - val_mae: 185.0228 - val_mse: 227610.8906\n",
      "Epoch 26/1000\n",
      "46800/46800 [==============================] - 9s 192us/sample - loss: 1657799.7131 - mae: 201.5141 - mse: 1657797.8750 - val_loss: 227490.7714 - val_mae: 179.8398 - val_mse: 227490.7031\n",
      "Epoch 27/1000\n",
      "46800/46800 [==============================] - 9s 194us/sample - loss: 1657751.1878 - mae: 200.5678 - mse: 1657750.6250 - val_loss: 227741.1226 - val_mae: 186.9910 - val_mse: 227741.0625\n",
      "Epoch 28/1000\n",
      "46800/46800 [==============================] - 9s 191us/sample - loss: 1657718.7078 - mae: 199.3857 - mse: 1657719.1250 - val_loss: 228955.4299 - val_mae: 203.9453 - val_mse: 228955.4844\n",
      "Epoch 29/1000\n",
      "46800/46800 [==============================] - 9s 191us/sample - loss: 1657601.3468 - mae: 201.3477 - mse: 1657602.3750 - val_loss: 228652.8927 - val_mae: 199.8192 - val_mse: 228652.9219\n",
      "TRAINING ENDED\n",
      "-------------------------------------------------------------------------------\n",
      "/n\n",
      "Strategy net profitability\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "104130.78693500519\n",
      "-------------------------------------------------------------------------------\n",
      "/n\n",
      "the first few reults of our predictions - to check they arent identical\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "[[119.32917  ]\n",
      " [ 76.45023  ]\n",
      " [225.5177   ]\n",
      " [267.44092  ]\n",
      " [234.56013  ]\n",
      " [  2.2193959]\n",
      " [294.22736  ]\n",
      " [168.81502  ]\n",
      " [102.73528  ]\n",
      " [ 75.158676 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_whole_model_assesment(build_tanh10_relu5_relu5_lin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X_train_pre = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X_train)\n",
    "model = LinearRegression().fit(X_train_pre, y_train.claim_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
